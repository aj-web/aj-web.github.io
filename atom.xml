<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>呢喃的博客</title>
  
  
  <link href="https://aj-web.github.io/atom.xml" rel="self"/>
  
  <link href="https://aj-web.github.io/"/>
  <updated>2021-12-29T03:56:39.404Z</updated>
  <id>https://aj-web.github.io/</id>
  
  <author>
    <name>ninan</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>JVM-5：JVM底层学习</title>
    <link href="https://aj-web.github.io/%E6%89%8B%E5%86%99JVM%E7%AC%94%E8%AE%B0/"/>
    <id>https://aj-web.github.io/%E6%89%8B%E5%86%99JVM%E7%AC%94%E8%AE%B0/</id>
    <published>2021-11-26T16:00:00.000Z</published>
    <updated>2021-12-29T03:56:39.404Z</updated>
    
    <content type="html"><![CDATA[<hr><blockquote><p>前言：Java创建线程的能力，实际上是通过调用操作系统的能力实现的</p></blockquote><span id="more"></span><h2 id="Linux多线程机制："><a href="#Linux多线程机制：" class="headerlink" title="Linux多线程机制："></a>Linux多线程机制：</h2><ul><li><p>Linux创建线程：<code>pthread_create</code>  </p></li><li><p>等待子线程运行结束： <code>pthread_join</code></p></li><li><p>线程属性：<code>pthread_attr_int</code>,<code>pthread_attr_destory</code></p></li><li><p>线程栈大小：默认1M，可以修改</p></li><li><p>运行机制有两种：1.join机制   2.Detach机制：线程执行完了会自动释放资源（java中的线程运行机制就是Detach机制），又称分离线程</p></li><li><p>使用join机制时获取线程结果有两种，一种是通过<code>pthread_join</code>接收线程的执行结果，或者是在子线程中通过<code>pthread_exit</code>来退出子线程同时返回一个结果，但是由于JVM的线程是Detach机制创建的，所以并不能用上面两种方案实现</p></li><li><p>如果在Linux中创建线程使用Detach机制，那么<code>pthread_join</code>是无效的，早期Linux版本会报错。那么这个时候如何获取线程的执行结果呢？答案就是通过修改共享内存上变量的值</p><p>我们都知道Java创建的线程其实是调用的操作系统的能力，那么从java中的线程对象到OS的线程对象，依次经过以下对应关系，而Linux线程操作的结果，就会存储在JVM的Java Thread对象中的_VM_result_2中，这个对象就可以理解为共享内存</p><p><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/202111271645498.png"></p><p><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/202111271641514.png" alt="线程对象"></p><h1 id="关于STW"><a href="#关于STW" class="headerlink" title="关于STW"></a>关于STW</h1><ol><li><p>GC的时候会触发STW，暂停所有用户的线程，但是暂停用户线程的前提是用户到达了安全点</p></li><li><p>常见的安全点有： 1.方法返回之前  2.调用某个方法之后  3.抛出异常的位置  4.循环的末尾</p></li></ol><p><strong>GC时间=垃圾回收的时间+线程到达安全点的时间</strong></p><p>那么安全点到底是什么？这个得去JVM的源码中看下，其实安全点的本质就是一个4字节的可读可写可执行的内存区域，其实是1字节，但是为了对齐就申请了4字节，当正常执行时，线程运行到某个特殊方法对安全点进行操作，此时不会抛出异常，<em><strong>其实本质没有进行啥操作但是重要的是没有抛出信号（异常）</strong></em>，但是如果GC需要阻塞线程的时候，JVM会把安全点修改成为不可读不可写不可执行，这样当线程运行到操作安全点的方法时就会触发异常。抛出信号SIGSEGV，JVM中会提前注册了这个信号，所以抛出信号后会进行宿舍，执行阻塞的操作是CAS实现的。</p><p><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/202111271724397.png"></p><p>happens-befor——先行发生原则</p></li></ul>]]></content>
    
    
    <summary type="html">&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;前言：Java创建线程的能力，实际上是通过调用操作系统的能力实现的&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    
    <category term="JVM" scheme="https://aj-web.github.io/tags/JVM/"/>
    
  </entry>
  
  <entry>
    <title>JVM-4：垃圾回收器与三色标记</title>
    <link href="https://aj-web.github.io/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8%E4%B8%8E%E4%B8%89%E8%89%B2%E6%A0%87%E8%AE%B0/"/>
    <id>https://aj-web.github.io/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8%E4%B8%8E%E4%B8%89%E8%89%B2%E6%A0%87%E8%AE%B0/</id>
    <published>2021-11-19T16:00:00.000Z</published>
    <updated>2021-12-29T03:56:39.411Z</updated>
    
    <content type="html"><![CDATA[<hr><blockquote><p>前言：对象的生死由垃圾回收算法和垃圾回收器来判断决定。</p></blockquote><span id="more"></span><h1 id="1-垃圾回收算法"><a href="#1-垃圾回收算法" class="headerlink" title="1.垃圾回收算法"></a>1.垃圾回收算法</h1><p>常见的垃圾回收算法主要有以下几种，但都是基于分带收集理论来实现的<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E7%AE%97%E6%B3%95.png" alt="垃圾回收算法"></p><h3 id="1-1分代收集理论"><a href="#1-1分代收集理论" class="headerlink" title="1.1分代收集理论"></a>1.1分代收集理论</h3><ul><li>当前虚拟机的垃圾收集都采用分代收集算法，这种算法没有什么新的思想，只是根据对象存活周期的不同将内存分为几块。一般将java堆分为新生代和老年代，这样我们就可以根据各个年代的特点选择合适的垃圾收集算法。</li><li>比如在新生代中，每次收集都会有大量象(近99%)死去，所以可以选择复制算，只需要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择“标记-清除”或“标记-整理”算法进行垃圾收集。注意，“标记-清除”或“标记-整理”算法会比复制算法慢10倍以上。</li></ul><h3 id="1-2标记-复制算法"><a href="#1-2标记-复制算法" class="headerlink" title="1.2标记-复制算法"></a>1.2标记-复制算法</h3><ul><li>为了解决效率问题，“复制”收集算法出现了。它可以将内存分为大小相同的两块，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。<h3 id="1-3标记-清除算法"><a href="#1-3标记-清除算法" class="headerlink" title="1.3标记-清除算法"></a>1.3标记-清除算法</h3></li><li>算法分为“标记”和“清除”阶段：标记存活的对象， 统一回收所有未被标记的对象(一般选择这种)；也可以反过来，标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象 。它是最基础的收集算法，比较简单，但是会带来两个明显的问题：<br>效率问题  (如果需要标记的对象太多，效率不高)<br>空间问题（标记清除后会产生大量不连续的碎片）</li></ul><h3 id="1-4标记-整理算法"><a href="#1-4标记-整理算法" class="headerlink" title="1.4标记-整理算法"></a>1.4标记-整理算法</h3><ul><li>根据老年代的特点特出的一种标记算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象回收，而是让所有存活的对象向一端移动，然后直接清理掉端边界以外的内存。</li></ul><h1 id="2-垃圾收集器"><a href="#2-垃圾收集器" class="headerlink" title="2.垃圾收集器"></a>2.垃圾收集器</h1><p><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8.png" alt="垃圾收集器"><br>如果说收集算法是内存回收的方法论，那么垃圾收集器就是内存回收的具体实现。<br>虽然我们对各个收集器进行比较，但并非为了挑选出一个最好的收集器。因为直到现在为止还没有最好的垃圾收集器出现，更加没有万能的垃圾收集器，我们能做的就是根据具体应用场景选择适合自己的垃圾收集器。试想一下：如果有一种四海之内、任何场景下都适用的完美收集器存在，那么我们的Java虚拟机就不会实现那么多不同的垃圾收集器了。</p><h3 id="2-1Serial收集器-XX-UseSerialGC-XX-UseSerialOldGC"><a href="#2-1Serial收集器-XX-UseSerialGC-XX-UseSerialOldGC" class="headerlink" title="2.1Serial收集器(-XX:+UseSerialGC  -XX:+UseSerialOldGC)"></a>2.1Serial收集器(-XX:+UseSerialGC  -XX:+UseSerialOldGC)</h3><ol><li>Serial（串行）收集器是最基本、历史最悠久的垃圾收集器了。大家看名字就知道这个收集器是一个单线程收集器了。它的 “单线程” 的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程（ “Stop The World” ），直到它收集结束。</li><li><em><strong>新生代采用复制算法，老年代采用标记-整理算法</strong></em>。</li><li>虚拟机的设计者们当然知道Stop The World带来的不良用户体验，所以在后续的垃圾收集器设计中停顿时间在不断缩短（仍然还有停顿，寻找最优秀的垃圾收集器的过程仍然在继续）。</li><li>但是Serial收集器有没有优于其他垃圾收集器的地方呢？当然有，它简单而高效（与其他收集器的单线程相比）。<em><strong>Serial收集器由于没有线程交互的开销，自然可以获得很高的单线程收集效率</strong></em>。</li><li>Serial Old收集器是Serial收集器的老年代版本，它同样是一个单线程收集器。它主要有两大用途：一种用途是在JDK1.5以及以前的版本中与Parallel Scavenge收集器搭配使用，另一种用途是作为CMS收集器的后备方案。<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/202111261034951.png" alt="Serial垃圾收集器"></li></ol><h3 id="2-2Parallel-Scavenge收集器-XX-UseParallelGC-年轻代-XX-UseParallelOldGC-老年代"><a href="#2-2Parallel-Scavenge收集器-XX-UseParallelGC-年轻代-XX-UseParallelOldGC-老年代" class="headerlink" title="2.2Parallel Scavenge收集器(-XX:+UseParallelGC(年轻代),-XX:+UseParallelOldGC(老年代))"></a>2.2Parallel Scavenge收集器(-XX:+UseParallelGC(年轻代),-XX:+UseParallelOldGC(老年代))</h3><ol><li>Parallel收集器其实就是Serial收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）和Serial收集器类似。默认的收集线程数跟cpu核数相同，当然也可以用参数(-XX:ParallelGCThreads)指定收集线程数，但是一般不推荐修改。</li><li>Parallel Scavenge收集器关注点是吞吐量（高效率的利用CPU）。CMS等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体验）。所谓<em><strong>吞吐量就是CPU中用于运行用户代码的时间与CPU总消耗时间的比值</strong></em>。 Parallel Scavenge收集器提供了很多参数供用户找到最合适的停顿时间或最大吞吐量，如果对于收集器运作不太了解的话，可以选择把内存管理优化交给虚拟机去完成也是一个不错的选择。</li><li><em><strong>新生代采用复制算法，老年代采用标记-整理算法</strong></em>。<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/202111261041214.png" alt="Parallel垃圾收集器"></li></ol><h3 id="2-3ParNew收集器-XX-UseParNewGC"><a href="#2-3ParNew收集器-XX-UseParNewGC" class="headerlink" title="2.3ParNew收集器(-XX:+UseParNewGC)"></a>2.3ParNew收集器(-XX:+UseParNewGC)</h3><p>ParNew收集器其实跟Parallel收集器很类似，区别主要在于它可以和CMS收集器配合使用。 新生代采用复制算法，老年代采用标记-整理算法。它是许多运行在Server模式下的虚拟机的首要选择，除了Serial收集器外，只有它能与CMS收集器（真正意义上的并发收集器，后面会介绍到）配合工作。<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/202111261041214.png" alt="Parallel垃圾收集器"></p><h3 id="2-4-CMS收集器-XX-UseConcMarkSweepGC-old"><a href="#2-4-CMS收集器-XX-UseConcMarkSweepGC-old" class="headerlink" title="2.4 CMS收集器(-XX:+UseConcMarkSweepGC(old))"></a>2.4 CMS收集器(-XX:+UseConcMarkSweepGC(old))</h3><p>CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。它非常符合在注重用户体验的应用上使用，它是HotSpot虚拟机第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。<br>从名字中的Mark Sweep这两个词可以看出，CMS收集器是一种 “标记-清除”算法实现的，它的运作过程相比于前面几种垃圾收集器来说更加复杂一些。整个过程分为四个步骤：</p><ul><li>初始标记： 暂停所有的其他线程(STW)，并记录下gc roots直接能引用的对象，速度很快。</li><li>并发标记： 并发标记阶段就是从GC Roots的直接关联对象开始遍历整个对象图的过程， 这个过程耗时较长但是不需要停顿用户线程， 可以与垃圾收集线程一起并发运行。因为用户程序继续运行，可能会有导致已经标记过的对象状态发生改变。</li><li>重新标记： 重新标记阶段就是为了修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段的时间稍长，远远比并发标记阶段时间短。主要用到三色标记里的增量更新算法(见下面详解)做重新标记。</li><li>并发清理： 开启用户线程，同时GC线程开始对未标记的区域做清扫。这个阶段如果有新增对象会被标记为黑色不做任何处理(见下面三色标记算法详解)。</li><li>并发重置：重置本次GC过程中的标记数据。<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/202111261052215.png" alt="CMS垃圾收集器"><br>从它的名字就可以看出它是一款优秀的垃圾收集器，主要优点：并发收集、低停顿。但是它有下面几个明显的缺点：</li></ul><ol><li>对CPU资源敏感（会和服务抢资源）；</li><li>无法处理浮动垃圾(在并发标记和并发清理阶段又产生垃圾，这种浮动垃圾只能等到下一次gc再清理了)；</li><li>它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生，当然通过参数-XX:+UseCMSCompactAtFullCollection可以让jvm在执行完标记清除后再做整理</li><li><em><strong>执行过程中的不确定性，会存在上一次垃圾回收还没执行完，然后垃圾回收又被触发的情况，特别是在并发标记和并发清理阶段会出现，一边回收，系统一边运行，也许没回收完就再次触发full gc，也就是”concurrent mode failure”，此时会进入stop the world，用serial old垃圾收集器来回收</strong></em></li></ol><p><em><strong>CMS的相关核心参数</strong></em></p><ul><li>-XX:+UseConcMarkSweepGC：启用cms </li><li>-XX:ConcGCThreads：并发的GC线程数</li><li>-XX:+UseCMSCompactAtFullCollection：FullGC之后做压缩整理（减少碎片）</li><li>-XX:CMSFullGCsBeforeCompaction：多少次FullGC之后压缩一次，默认是0，代表每次FullGC后都会压缩一次  </li><li>-XX:CMSInitiatingOccupancyFraction: 当老年代使用达到该比例时会触发FullGC（默认是92，这是百分比）</li><li>-XX:+UseCMSInitiatingOccupancyOnly：只使用设定的回收阈值(-XX:CMSInitiatingOccupancyFraction设定的值)，如果不指定，JVM仅在第一次使用设定值，后续则会自动调整</li><li>-XX:+CMSScavengeBeforeRemark：在CMS GC前启动一次minor gc，降低CMS GC标记阶段(也会对年轻代一起做标记，如果在minor gc就干掉了很多对垃圾对象，标记阶段就会减少一些标记时间)时的开销，一般CMS的GC耗时 80%都在标记阶段</li><li>-XX:+CMSParallellnitialMarkEnabled：表示在初始标记的时候多线程执行，缩短STW</li><li>-XX:+CMSParallelRemarkEnabled：在重新标记的时候多线程执行，缩短STW;</li></ul><p><em><strong>CMS调优建议：</strong></em><br>打开-XX:+CMSParallellnitialMarkEnabled在初始标记的时候采用多线程，<br>-XX:+CMSParallelRemarkEnabled，打开<br>-XX:+UseCMSInitiatingOccupancyOnly，打开</p><h1 id="3-亿级流量电商系统如何优化JVM参数设置-ParNew-CMS"><a href="#3-亿级流量电商系统如何优化JVM参数设置-ParNew-CMS" class="headerlink" title="3.亿级流量电商系统如何优化JVM参数设置(ParNew+CMS)"></a>3.亿级流量电商系统如何优化JVM参数设置(ParNew+CMS)</h1><p>大型电商系统后端现在一般都是拆分为多个子系统部署的，比如，商品系统，库存系统，订单系统，促销系统，会员系统等等。<br>我们这里以比较核心的订单系统为例<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/202111261101658.png"><br>对于8G内存，我们一般是分配4G内存给JVM，正常的JVM参数配置如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-Xms3072M -Xmx3072M -Xss1M -XX:MetaspaceSize=256M -XX:MaxMetaspaceSize=256M  -XX:SurvivorRatio=8</span><br></pre></td></tr></table></figure><p>这样设置可能会由于动态对象年龄判断原则导致频繁full gc<br>于是更新为以下   </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-Xms3072M -Xmx3072M -Xmn2048M -Xss1M  -XX:MetaspaceSize=256M -XX:MaxMetaspaceSize=256M  -XX:SurvivorRatio=8 </span><br><span class="line">-XX:MaxTenuringThreshold=5 -XX:PretenureSizeThreshold=1M </span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/202111261124073.png"><br>这样就降低了因为对象动态年龄判断原则导致的对象频繁进入老年代的问题，其实很多优化无非就是让短期存活的对象尽量都留在survivor里，不要进入老年代，这样在minor gc的时候这些对象都会被回收，不会进到老年代从而导致full gc。<br>对于对象年龄应该为多少才移动到老年代比较合适，本例中一次minor gc要间隔二三十秒，大多数对象一般在几秒内就会变为垃圾，完全可以将默认的15岁改小一点，比如改为5，那么意味着对象要经过5次minor gc才会进入老年代，整个时间也有一两分钟了，如果对象这么长时间都没被回收，完全可以认为这些对象是会存活的比较长的对象，可以移动到老年代，而不是继续一直占用survivor区空间。<br>对于多大的对象直接进入老年代(参数-XX:PretenureSizeThreshold)，这个一般可以结合你自己系统看下有没有什么大对象生成，预估下大对象的大小，一般来说设置为1M就差不多了，很少有超过1M的大对象，这些对象一般就是你系统初始化分配的缓存对象，比如大的缓存List，Map之类的对象。<br>可以适当调整JVM参数如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-Xms3072M -Xmx3072M -Xmn2048M -Xss1M  -XX:MetaspaceSize=256M -XX:MaxMetaspaceSize=256M  -XX:SurvivorRatio=8 </span><br><span class="line">-XX:MaxTenuringThreshold=5 -XX:PretenureSizeThreshold=1M </span><br></pre></td></tr></table></figure><p>对于JDK8默认的垃圾回收器是-XX:+UseParallelGC(年轻代)和-XX:+UseParallelOldGC(老年代)，如果内存较大(超过4个G，只是经验值)，系统对停顿时间比较敏感，我们可以使用ParNew+CMS(-XX:+UseParNewGC -XX:+UseConcMarkSweepGC)<br>对于老年代CMS的参数如何设置我们可以思考下，首先我们想下当前这个系统有哪些对象可能会长期存活躲过5次以上minor gc最终进入老年代。<br>无非就是那些Spring容器里的Bean，线程池对象，一些初始化缓存数据对象等，这些加起来充其量也就几十MB。<br>还有就是某次minor gc完了之后还有超过一两百M的对象存活，那么就会直接进入老年代，比如突然某一秒瞬间要处理五六百单，那么每秒生成的对象可能有一百多M，再加上整个系统可能压力剧增，一个订单要好几秒才能处理完，下一秒可能又有很多订单过来。<br>我们可以估算下大概每隔五六分钟出现一次这样的情况，那么大概半小时到一小时之间就可能因为老年代满了触发一次Full GC，Full GC的触发条件还有我们之前说过的老年代空间分配担保机制，历次的minor gc挪动到老年代的对象大小肯定是非常小的，所以几乎不会在minor gc触发之前由于老年代空间分配担保失败而产生full gc，其实在半小时后发生full gc，这时候已经过了抢购的最高峰期，后续可能几小时才做一次FullGC。<br>对于碎片整理，因为都是1小时或几小时才做一次FullGC，是可以每做完一次就开始碎片整理，或者两到三次之后再做一次也行。<br>综上，只要年轻代参数设置合理，老年代CMS的参数设置基本都可以用默认值，如下所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-Xms3072M -Xmx3072M -Xmn2048M -Xss1M  -XX:MetaspaceSize=256M -XX:MaxMetaspaceSize=256M  -XX:SurvivorRatio=8 </span><br><span class="line">-XX:MaxTenuringThreshold=5 -XX:PretenureSizeThreshold=1M -XX:+UseParNewGC -XX:+UseConcMarkSweepGC </span><br><span class="line">-XX:CMSInitiatingOccupancyFraction=92 -XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=3</span><br></pre></td></tr></table></figure><h1 id="4-三色标记"><a href="#4-三色标记" class="headerlink" title="4.三色标记"></a>4.三色标记</h1><p>在并发标记的过程中，因为标记期间应用线程还在继续跑，对象间的引用可能发生变化，多标和漏标的情况就有可能发生。<br>这里我们引入“三色标记”来给大家解释下，把Gcroots可达性分析遍历对象过程中遇到的对象， 按照“是否访问过”这个条件标记成以下三种颜色：</p><ul><li>黑色： 表示对象已经被垃圾收集器访问过， 且这个对象的所有引用都已经扫描过。 黑色的对象代表已经扫描过， 它是安全存活的， 如果有其他对象引用指向了黑色对象， 无须重新扫描一遍。 黑色对象不可能直接（不经过灰色对象） 指向某个白色对象。</li><li>灰色： 表示对象已经被垃圾收集器访问过， 但这个对象上至少存在一个引用还没有被扫描过。</li><li>白色： 表示对象尚未被垃圾收集器访问过。 显然在可达性分析刚刚开始的阶段， 所有的对象都是白色的， 若在分析结束的阶段， 仍然是白色的对象， 即代表不可达。<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/202111261128924.png"></li></ul><h3 id="4-1多标-浮动垃圾"><a href="#4-1多标-浮动垃圾" class="headerlink" title="4.1多标-浮动垃圾"></a>4.1多标-浮动垃圾</h3><p>在并发标记过程中，如果由于方法运行结束导致部分局部变量(gcroot)被销毁，这个gcroot引用的对象之前又被扫描过(被标记为非垃圾对象)，那么本轮GC不会回收这部分内存。这部分本应该回收但是没有回收到的内存，被称之为“浮动垃圾”。浮动垃圾并不会影响垃圾回收的正确性，只是需要等到下一轮垃圾回收中才被清除。<br>另外，针对并发标记(还有并发清理)开始后产生的新对象，通常的做法是直接全部当成黑色，本轮不会进行清除。这部分对象期间可能也会变为垃圾，这也算是浮动垃圾的一部分。</p><p>多标个人理解：开始初始标记到一个只有直接应用的对象，然后开始并行标记，这个对象也没有其他引用，所以对象变成了黑色，在后续过程中不会回收这个对像</p><p><em><strong>漏标-读写屏障</strong></em><br>A对象引用了B对象，B对象引用了C对象，开始标记时，标记了A对象，A对象变为黑色，并发标记时B对象不引用C对象了，所以扫到B变黑色，C变白色，但是此时A对象又引用到C对象了，但是A之前扫描过了，不会再扫A，所以C还是垃圾对象</p><h3 id="4-2两种解决方案：-增量更新（Incremental-Update）-和原始快照（Snapshot-At-The-Beginning，SATB）-。"><a href="#4-2两种解决方案：-增量更新（Incremental-Update）-和原始快照（Snapshot-At-The-Beginning，SATB）-。" class="headerlink" title="4.2两种解决方案： 增量更新（Incremental Update） 和原始快照（Snapshot At The Beginning，SATB） 。"></a>4.2两种解决方案： 增量更新（Incremental Update） 和原始快照（Snapshot At The Beginning，SATB） 。</h3><p><em><strong>增量更新</strong></em>就是当黑色对象插入新的指向白色对象的引用关系时， 就将这个新插入的引用记录下来， 等并发扫描结束之后， 再将这些记录过的引用关系中的黑色对象为根， 重新扫描一次。 这可以简化理解为， 黑色对象一旦新插入了指向白色对象的引用之后， 它就变回灰色对象了。</p><p><em><strong>原始快照</strong></em>就是当灰色对象要删除指向白色对象的引用关系时， 就将这个要删除的引用记录下来， 在并发扫描结束之后， 再将这些记录过的引用关系中的灰色对象为根， 重新扫描一次，这样就能扫描到白色的对象，将白色对象直接标记为黑色(目的就是让这种对象在本轮gc清理中能存活下来，待下一轮gc的时候重新扫描，这个对象也有可能是浮动垃圾)<br>以上无论是对引用关系记录的插入还是删除， 虚拟机的记录操作都是通过写屏障实现的。</p><p><em><strong>写屏障</strong></em><br>给某个对象的成员变量赋值时，其底层代码大概长这样：<br>所谓的写屏障，其实就是指在赋值操作前后，加入一些处理（可以参考AOP的概念）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">void oop_field_store(oop* field, oop new_value) &#123;  </span><br><span class="line">    pre_write_barrier(field);          // 写屏障-写前操作</span><br><span class="line">    *field = new_value; </span><br><span class="line">    post_write_barrier(field, value);  // 写屏障-写后操作</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em><strong>写屏障实现SATB</strong></em><br>当对象B的成员变量的引用发生变化时，比如引用消失（a.b.d = null），我们可以利用写屏障，将B原来成员变量的引用对象D记录下来</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">void post_write_barrier(oop* field, oop new_value) &#123;  </span><br><span class="line">    remark_set.add(new_value);  // 记录新引用的对象</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em><strong>写屏障实现增量更新</strong></em><br>当对象A的成员变量的引用发生变化时，比如新增引用（a.d = d），我们可以利用写屏障，将A新的成员变量引用对象D记录下来：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">void post_write_barrier(oop* field, oop new_value) &#123;  </span><br><span class="line">    remark_set.add(new_value);  // 记录新引用的对象</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>读屏障</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">oop oop_field_load(oop* field) &#123;</span><br><span class="line">    pre_load_barrier(field); // 读屏障-读取前操作</span><br><span class="line">    return *field;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void pre_load_barrier(oop* field) &#123;  </span><br><span class="line">    oop old_value = *field;</span><br><span class="line">    remark_set.add(old_value); // 记录读取到的对象</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em><strong>各个垃圾收集器解决浮动垃圾的办法：</strong></em><br>CMS：写屏障 + 增量更新<br>G1，Shenandoah：写屏障 + SATB<br>ZGC：读屏障</p><p>面试题：<br>为什么G1用SATB？CMS用增量更新？<br>我的理解：SATB相对增量更新效率会高(当然SATB可能造成更多的浮动垃圾)，因为不需要在重新标记阶段再次深度扫描被删除引用对象，而CMS对增量引用的根对象会做深度扫描，G1因为很多对象都位于不同的region，CMS就一块老年代区域，重新深度扫描对象的话G1的代价会比CMS高，所以G1选择SATB不深度扫描对象，只是简单标记，等到下一轮GC再深度扫描。</p><h1 id="5-记忆集与卡表"><a href="#5-记忆集与卡表" class="headerlink" title="5.记忆集与卡表"></a>5.记忆集与卡表</h1><p>在新生代做GCRoots可达性扫描过程中可能会碰到跨代引用的对象，这种如果又去对老年代再去扫描效率太低了。<br>为了解决跨代应用的问题，新生代可以引入记录集（Remember Set）的数据结构（记录从非收集区到收集区的指针集合），避免把整个老年代加入GCRoots扫描范围，，收集器只需通过记忆集判断出某一块非收集区域是否存在指向收集区域的指针即可，无需了解跨代引用指针的全部细节</p><p>记忆集是数据结构，hotspot用卡表实现了它，卡表用一个字节数组实现，里面每个元素对应标识的一块特定大小的内存块，称为卡页，每个卡页中可以包含多个对象，只要这个对象存在跨代指针，标志就变为1，如何实现，就是引用字段赋值时，更新卡表的表示，采用写屏障</p>]]></content>
    
    
    <summary type="html">&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;前言：对象的生死由垃圾回收算法和垃圾回收器来判断决定。&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    
    <category term="JVM" scheme="https://aj-web.github.io/tags/JVM/"/>
    
  </entry>
  
  <entry>
    <title>JVM-3:对象内存机制分配</title>
    <link href="https://aj-web.github.io/JVM%E5%AF%B9%E8%B1%A1%E5%86%85%E5%AD%98%E6%9C%BA%E5%88%B6%E5%88%86%E9%85%8D/"/>
    <id>https://aj-web.github.io/JVM%E5%AF%B9%E8%B1%A1%E5%86%85%E5%AD%98%E6%9C%BA%E5%88%B6%E5%88%86%E9%85%8D/</id>
    <published>2021-11-18T16:00:00.000Z</published>
    <updated>2021-12-29T03:56:39.410Z</updated>
    
    <content type="html"><![CDATA[<hr><blockquote><p>前言：思考一个问题，对象是如何创建的，对象创建的过程是怎样的？ 详情请看对象内存机制分配详解    </p></blockquote><span id="more"></span><h1 id="1-对象的创建总流程"><a href="#1-对象的创建总流程" class="headerlink" title="1. 对象的创建总流程"></a>1. 对象的创建总流程</h1><p>直接上我整理的图，按照这个图的顺序来讲解对象创建的流程<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/%E5%AF%B9%E8%B1%A1%E5%88%9B%E5%BB%BA.png" alt="对象创建"></p><h4 id="1-1-类加载检查"><a href="#1-1-类加载检查" class="headerlink" title="1.1 类加载检查"></a>1.1 类加载检查</h4><p>虚拟机遇到一条new指令时，首先将去检查这个指令的参数是否能在常量池中定位到一个类的符号引用，并且检查这个符号引用代表的类是否已被加载、解析和初始化过。如果没有，那必须先执行相应的类加载过程。<br>new指令对应到语言层面上讲是，new关键词、对象克隆、对象序列化等。</p><h4 id="1-2-分配内存"><a href="#1-2-分配内存" class="headerlink" title="1.2 分配内存"></a>1.2 分配内存</h4><p>(1)指针碰撞”（Bump the Pointer）：如果内存规整那么指针左边存放已经分配的内存，右边存放未分配的内粗你，指针移动一段等于对象大小的距离<br>(2)空闲列表”（Free List）：如果内存不规整，就需要有一个列表记录哪些内存可用,为什么会出现空闲列表的情况，垃圾回收中的标记清除<br>(3)多线程中，多个对象分配内存，如何确保每个对象都成功分配内存：<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/TLAB.png" alt="TLAB"><br>3.1:CAS（compare and swap）虚拟机采用CAS配上失败重试的方式保证更新操作的原子性来对分配内存空间的动作进行同步处理。<br>3.2:本地线程分配缓冲（Thread Local Allocation Buffer,TLAB），把内存分配的动作按照线程划分在不同的空间之中进行，即每个线程在Java堆中预先分配一小块内存。通过­XX:+/­-UseTLAB参数来设定虚拟机是否使用TLAB(JVM会默认开启­XX:+UseTLAB)，­XX:TLABSize 指定TLAB大小，默认大小为Eden的1%  </p><h4 id="1-3-初始化零值"><a href="#1-3-初始化零值" class="headerlink" title="1.3 初始化零值"></a>1.3 初始化零值</h4><p>内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头）， 如果使用TLAB，这一工作过程也可以提前至TLAB分配时进行。这一步操作保证了对象的实例字段在Java代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型    所对应的零值。</p><h4 id="1-4-设置对象头"><a href="#1-4-设置对象头" class="headerlink" title="1.4 设置对象头"></a>1.4 设置对象头</h4><p>对象在内存中存储的布局可以分为3块区域：对象头（Header）、 实例数据（Instance Data）和对齐填充（Padding），对象头详细如下<br>Mark Word：不同状态的对线头不一样，一般有对象的hashcode(25位)，分代年龄(4位),是否偏向头，锁标志位<br>Klass Points：开启指针压缩时占4个字节，关闭指针压缩时占8个字节<br>数组长度：当对象为数组的时候才有<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/%E5%AF%B9%E8%B1%A1%E5%A4%B4.png" alt="对象头"></p><h4 id="1-5-执行init方法"><a href="#1-5-执行init方法" class="headerlink" title="1.5 执行init方法"></a>1.5 执行init方法</h4><p>init方法是C++语言实现的，执行<init>方法，即对象按照程序员的意愿进行初始化。对应到语言层面上讲，<br>就是为属性赋值（注意，这与上面的赋零值不同，这是由程序员赋的值），和执行构造方法。   </p><h4 id="1-6-什么是指针压缩"><a href="#1-6-什么是指针压缩" class="headerlink" title="1.6 什么是指针压缩"></a>1.6 什么是指针压缩</h4><p>java对象的指针压缩？<br>1.在64位平台的HotSpot中使用32位指针(实际存储用64位)，内存使用会多出1.5倍左右，使用较大指针在主内存和缓存之间移动数据，占用较大宽带，同时GC也会承受较大压力<br>2.为了减少64位平台下内存的消耗，启用指针压缩功能<br>3.在jvm中，32位地址最大支持4G内存(2的32次方)，可以通过对对象指针的存入堆内存时压缩编码、取出到cpu寄存器后解码方式进行优化(对象指针在堆中是32位，在寄存器中是35位，2的35次方=32G)，使得jvm只用32位地址就可以支持更大的内存配置(小于等于32G)<br>4.堆内存小于4G时，不需要启用指针压缩，jvm会直接去除高32位地址，即使用低虚拟地址空间<br>5.堆内存大于32G时，压缩指针会失效，会强制使用64位(即8字节)来对java对象寻址，这就会出现1的问题，所以堆内存不要大于32G为好</p><h4 id="1-7-对齐填充："><a href="#1-7-对齐填充：" class="headerlink" title="1.7 对齐填充："></a>1.7 对齐填充：</h4><p>对于大部分处理器，对象以8字节整数倍来对齐填充都是最高效的存取方式</p><h1 id="2-对象内存分配"><a href="#2-对象内存分配" class="headerlink" title="2. 对象内存分配"></a>2. 对象内存分配</h1><p>上面我们讲了对象分配的方式，接着我们来介绍下对象具体如何分配的   </p><h4 id="2-1"><a href="#2-1" class="headerlink" title="2.1"></a>2.1</h4><ol><li>new一个对象时，会通过逃逸分析先判断对象能否分配在栈上，JVM通过逃逸分析确定该对象不会被外部访问。如果不会逃逸可以将该对象在栈上分配内存，这样可以减轻垃圾回收的压力<br>在栈上创建对象的时候，还会通过标量替换，来优化  </li></ol><ul><li>对象逃逸分析：就是分析对象动态作用域，当一个对象在方法中被定义后，它可能被外部方法所引用，例如作为调用参数传递到其他地方中。说白了就是判断这个对象是否只在一个方法中被使用</li><li>标量替换：通过逃逸分析确定该对象不会被外部访问，并且对象可以被进一步分解时，JVM不会创建该对象，而是将该对象成员变量分解若干个被这个方法使用的成员变量所代替，这些代替的成员变量在栈帧或寄存器上分配空间，这样就不会因为没有一大块连续空间导致对象内存不够分配</li><li>标量与聚合量：标量即不可被进一步分解的量，而JAVA的基本数据类型就是标量（如：int，long等基本数据类型以及reference类型等），标量的对立就是可以被进一步分解的量，而这种量称之为聚合量。而在JAVA中对象就是可以被进一步分解的聚合量。结论：栈上分配依赖于逃逸分析和标量替换   </li></ul><ol start="2"><li>能在栈上分配，则在栈上分配，否则在堆上进行分配   </li><li>在堆上进行分配时：对象优先分配在Eden区,如果是大对象(字符串，数组)，大对象大小在这个参数只在 Serial 和ParNew两个收集器下可以设置，会直接放进老年代   </li><li>不是大对象，会判断Eden区能否放下，不能的话，会执行minor GC，执行完还不能就会直接放入老年代   </li><li>不是大对象则会采用TLAB在堆中预先分配内存，或者直接分配，多线程可能CAS分配 </li></ol><h4 id="2-2-对象动态年龄判断"><a href="#2-2-对象动态年龄判断" class="headerlink" title="2.2 对象动态年龄判断"></a>2.2 对象动态年龄判断</h4><p>对象动态年龄判断机制一般是在minor gc之后触发的。当前存放对象的Survivor区中，一批对象的总大小大于Survivor区域内存大小的50%(-XX:TargetSurvivorRatio可以指定年龄最大值)，那么此时大于等于这批对象年龄最大值的对象，就可以直接进入老年代了。这个规则其实是希望那些可能是长期存活的对象，尽早进入老年代。</p><h4 id="2-3-老年代空间分配担保机制"><a href="#2-3-老年代空间分配担保机制" class="headerlink" title="2.3 老年代空间分配担保机制"></a>2.3 老年代空间分配担保机制</h4><p>年轻代每次minor GC之前都会判断如果老年代的可用空间小于年轻代里面所有对象的大小之和，就会看一个-XX:-HandlePromotionFailure”(jdk1.8默认就设置了)是否设置，如果有就会看历史minor GC之后进入老年代对象的平均大小是否小于老年代的可用内存，如果小于，代表执行minor GC即可，如果大于则执行full GC，没有设置参数则直接执行full GC，如果经过以上操作，对象不能全放进老年带，则OOM错误</p><h1 id="3-对象内存回收"><a href="#3-对象内存回收" class="headerlink" title="3. 对象内存回收"></a>3. 对象内存回收</h1><h4 id="3-1-引用计数法"><a href="#3-1-引用计数法" class="headerlink" title="3.1 引用计数法"></a>3.1 引用计数法</h4><p>给对象中添加一个引用计数器，每当有一个地方引用它，计数器就加1；当引用失效，计数器就减1；任何时候计数器为0的对象就是不可能再被使用的。解决不了对象相互循环引用的问题</p><h4 id="3-2-可达性算法"><a href="#3-2-可达性算法" class="headerlink" title="3.2 可达性算法"></a>3.2 可达性算法</h4><p>将“GC Roots” 对象作为起点，从这些节点开始向下搜索引用的对象，找到的对象都标记为非垃圾对象，其余未标记的对象都是垃圾对象，<br>GC Roots根节点：线程栈的本地变量、静态变量、本地方法栈的变量等等</p><ol><li>虚拟机栈（栈帧中的本地变量表）中引用的对象</li><li>本地方法栈中JNI（即一般说的Native方法）引用的对象</li><li>方法区中类静态属性引用的对象</li><li>方法区中常量引用的对象</li></ol><h4 id="3-3-常见引用类型"><a href="#3-3-常见引用类型" class="headerlink" title="3.3 常见引用类型"></a>3.3 常见引用类型</h4><p>java的引用类型一般分为四种：强引用、软引用、弱引用、虚引用   </p><ul><li>强引用：普通的变量引用，例如new 对象   </li><li>软应用：GC时不会被主动回收，除非GC后的内存还是不够分配对象，那么此时就会回收软引用，软引用可用来实现内存敏感的高速缓存  </li><li>弱引用：GC会直接回收  </li><li>虚引用：几乎不用  </li></ul><h1 id="4-finalize-方法最终判定对象是否存活"><a href="#4-finalize-方法最终判定对象是否存活" class="headerlink" title="4. finalize()方法最终判定对象是否存活"></a>4. finalize()方法最终判定对象是否存活</h1><p>gc后无用的对象会被标记，然后进行筛选，如果对象没有覆盖ginalize方法，那么对象被直接回收，覆盖了finalize方法后，如果方法中对象被引用或者引用别的对象，那么就不会被回收</p><h1 id="5-如何判断一个类是无用的类"><a href="#5-如何判断一个类是无用的类" class="headerlink" title="5. 如何判断一个类是无用的类"></a>5. 如何判断一个类是无用的类</h1><p>类需要同时满足下面3个条件才能算是 “无用的类” ：<br>无对象实力  classloder被回收  Class对象没有被以用</p><ol><li>该类所有的对象实例都已经被回收，也就是 Java 堆中不存在该类的任何实例。</li><li>加载该类的 ClassLoader 已经被回收。</li><li>该类对应的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">JVM指令：</span><br><span class="line">1.本地线程分配缓冲（Thread Local Allocation Buffer,TLAB默认开启）：­XX:+/­-UseTLAB，­XX:TLABSize 指定TLAB大小</span><br><span class="line">2.指针压缩(JDK1.6默认开启):-XX:+/-UseCompressedOops(默认开启)</span><br><span class="line">3.逃逸分析(JDK1.7默认开启)：-XX:+DoEscapeAnalysis</span><br><span class="line">4.标量替换(JDK1.7默认开启)：-XX:+EliminateAllocations</span><br><span class="line">5.Eden与Survivor区占比8:1:1自动变化(默认开启):-XX:+/-UseAdaptiveSizePolicy</span><br><span class="line">6.设置大对象大小(SerialGC)：-XX:PretenureSizeThreshold=1000000 (单位是字节)  -XX:+UseSerialGC  </span><br><span class="line">7.设置分代年龄最大值:(-XX:TargetSurvivorRatio)</span><br><span class="line">8.设置空间分配担保参数：-XX:-HandlePromotionFailure</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;前言：思考一个问题，对象是如何创建的，对象创建的过程是怎样的？ 详情请看对象内存机制分配详解    &lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    
    <category term="JVM" scheme="https://aj-web.github.io/tags/JVM/"/>
    
  </entry>
  
  <entry>
    <title>JVM-2：类加载机制解析</title>
    <link href="https://aj-web.github.io/JVM%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%E8%A7%A3%E6%9E%90/"/>
    <id>https://aj-web.github.io/JVM%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%E8%A7%A3%E6%9E%90/</id>
    <published>2021-11-17T16:00:00.000Z</published>
    <updated>2021-12-29T03:56:39.413Z</updated>
    
    <content type="html"><![CDATA[<hr><h1 id="1-1类加载机制"><a href="#1-1类加载机制" class="headerlink" title="1.1类加载机制"></a>1.1类加载机制</h1><blockquote><p>类加载步骤：加载 &gt; &gt; 验证 &gt; &gt; 准备 &gt; &gt; 解析 &gt; &gt; 初始化 &gt; &gt; 使用 &gt; &gt; 卸载</p></blockquote><span id="more"></span><ul><li>加载：在硬盘上查找并通过IO读入字节码文件，使用到类时才会加载，例如调用类的main()方法，new对象等等，在加载阶段会在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口</li><li>验证：校验字节码文件的正确性</li><li>准备：给类的静态变量（类变量）分配内存，并赋予默认值</li><li>解析：将符号引用替换为直接引用，该阶段会把一些静态方法(符号引用，比如main()方法)替换为指向数据所存内存的指针或句柄等(直接引用)，这是所谓的静态链接过程(类加载期间完成)，动态链接是在程序运行期间完成的将符号引用替换为直接引用</li><li>初始化:对类的静态变量初始化为指定的值，执行静态代码块</li></ul><blockquote><p>以上是类加载的5个阶段，那么除了类加载的5个阶段，还有：<code>使用</code>，<code>卸载</code>两个阶段<br>也就是类的生命周期=类加载+<code>使用</code>+<code>卸载</code></p></blockquote><h3 id="1-1类加载机制拓展理解"><a href="#1-1类加载机制拓展理解" class="headerlink" title="1.1类加载机制拓展理解"></a>1.1类加载机制拓展理解</h3><p>上面的基本概念是各网站都能搜到的，我们再结合自己进行拓展，理解一下：</p><p><strong>加载：</strong></p><ol><li>加载会通过限定名(可以简单理解为类名)获取到类的二进制字节流</li><li>将二进制字节文件的数据放到方法区，然后在堆中生产一个代表这个类的java.lang.Class对象，Class 对象封装了类在方法区内的数据结构，并且向开发者提供了访问方法区内的数据结构的接口</li></ol><p>所以我们可以认为，开发中用的是Class对象，但是当我们想要这个类的某些信息的时候，我们需要通过这个Class对象，到方法区中去找。例如类的<code>元数据</code>和<code>方法信息(继承信息、成员变量、静态变量、成员方法、构造函数)</code>。那么如何找到，就涉及到对象头中的一个Klass point:类型指针，通过类型指针来找到方法区中的元数据等。</p><p><strong>验证：</strong></p><ol><li>文件格式验证：（class文件来源不唯一(自己也可以手写)，有可能格式正确损坏虚拟机）</li><li>元数据验证：（是否符合类的定义规范，例如是否继承java.lang.Object）</li><li>字节码验证：（类中方法的控制流是否合法）</li><li>符号引用验证：（转换为直接引用动作是否合法）</li></ol><p><strong>准备：</strong></p><ol><li>为类变量分配内存，赋予默认值</li><li>实例变量会在创建对象过程中一起被分配，详情看下一章</li></ol><p><strong>解析：</strong></p><ol><li>将符号引用替换为直接引用</li><li>符号引用（Symbolic References）：符号引用以一组符号来描述所引用的目标，符号可以是任何形式的字面量，只要使用时能无歧义地定位到目标即可。</li><li>直接引用（Direct References）：直接引用可以是直接指向目标的指针、相对偏移量或是一个能间接定位到目标的句柄。如果有了直接引用，那么引用的目标一定是已经存在于内存中。</li></ol><p>上面我们已经知道了，类的元数据和方法信息是存在方法区的，那么方法区可能存在符号引用，这个时候就需要进行解析了，通常有：1.类或接口的解析2.字段解析3.类方法解析4.接口方法解析，解析之后，针对我们解析到的内容，可能还需要进行上面的加载验证准备步骤</p><p><strong>初始化</strong></p><ol><li>为类变量赋予默认值</li><li>执行静态代码块，静态方法</li><li>执行构造方法</li></ol><p>执行顺序</p><ul><li>先加载父类的静态代码块和静态变量 这两个加载的顺序与代码顺序有关</li><li>加载子类的静态代码块和静态变量  加载的顺序也与位置有关</li><li>加载父类的变量和语句块</li><li>加载父类的构造方法</li><li>加载子类的变量和语句块</li><li>加载子类的构造方法</li></ul><h1 id="2-虚拟机的内存分配情况"><a href="#2-虚拟机的内存分配情况" class="headerlink" title="2.虚拟机的内存分配情况"></a>2.虚拟机的内存分配情况</h1><ol><li>虚拟机栈：每个class类对应一个虚拟机栈帧（组成：局部变量表、操作数栈、返回地址、动态链接），类私有</li><li>堆：存放对象</li><li>方法区：存放类信息、常量、类变量、即时编译器编译后的代码</li><li>常量池：是方法区的一部分，主要有字面量（常量和字符串）和符号引用（类和接口的符号引用、字段的名称和描述的符号引用、方法的名称和描述的符号引用）</li></ol><h1 id="3-类加载器和双亲委派机制"><a href="#3-类加载器和双亲委派机制" class="headerlink" title="3.类加载器和双亲委派机制"></a>3.类加载器和双亲委派机制</h1><h3 id="3-1类加载过程主要是通过类加载器来实现的，Java里有如下几种类加载器"><a href="#3-1类加载过程主要是通过类加载器来实现的，Java里有如下几种类加载器" class="headerlink" title="3.1类加载过程主要是通过类加载器来实现的，Java里有如下几种类加载器"></a>3.1类加载过程主要是通过类加载器来实现的，Java里有如下几种类加载器</h3><ol><li>引导类加载器：负责加载支撑JVM运行的位于JRE的lib目录下的核心类库，比如rt.jar、charsets.jar等</li><li>扩展类加载器：负责加载支撑JVM运行的位于JRE的lib目录下的ext扩展目录中的JAR类包</li><li>应用程序类加载器：负责加载ClassPath路径下的类包，主要就是加载你自己写的那些类</li><li>自定义加载器：负责加载用户自定义路径下的类包</li></ol><h3 id="3-2类加载器初始化过程："><a href="#3-2类加载器初始化过程：" class="headerlink" title="3.2类加载器初始化过程："></a>3.2类加载器初始化过程：</h3><p>参见类运行加载全过程图可知其中会创建JVM启动器实例sun.misc.Launcher。<br>在Launcher构造方法内部，其创建了两个类加载器，分别是sun.misc.Launcher.ExtClassLoader(扩展类加载器)和sun.misc.Launcher.AppClassLoader(应用类加载器)。<br>JVM默认使用Launcher的getClassLoader()方法返回的类加载器AppClassLoader的实例加载我们的应用程序。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">//Launcher的构造方法</span><br><span class="line">public Launcher() &#123;</span><br><span class="line">    Launcher.ExtClassLoader var1;</span><br><span class="line">    try &#123;</span><br><span class="line">        //构造扩展类加载器，在构造的过程中将其父加载器设置为null</span><br><span class="line">        var1 = Launcher.ExtClassLoader.getExtClassLoader();</span><br><span class="line">    &#125; catch (IOException var10) &#123;</span><br><span class="line">        throw new InternalError(&quot;Could not create extension class loader&quot;, var10);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    try &#123;</span><br><span class="line">        //构造应用类加载器，在构造的过程中将其父加载器设置为ExtClassLoader，</span><br><span class="line">        //Launcher的loader属性值是AppClassLoader，我们一般都是用这个类加载器来加载我们自己写的应用程序</span><br><span class="line">        this.loader = Launcher.AppClassLoader.getAppClassLoader(var1);</span><br><span class="line">    &#125; catch (IOException var9) &#123;</span><br><span class="line">        throw new InternalError(&quot;Could not create application class loader&quot;, var9);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Thread.currentThread().setContextClassLoader(this.loader);</span><br><span class="line">    String var2 = System.getProperty(&quot;java.security.manager&quot;);</span><br><span class="line">    。。。 。。。 //省略一些不需关注代码</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-3双亲委派机制"><a href="#3-3双亲委派机制" class="headerlink" title="3.3双亲委派机制"></a>3.3双亲委派机制</h3><p>JVM类加载器是有亲子层级结构的，如下图<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/%E5%8F%8C%E4%BA%B2%E5%A7%94%E6%B4%BE.png" alt="双亲委派机制"></p><p>这里类加载其实就有一个双亲委派机制，加载某个类时会先委托父加载器寻找目标类，找不到再委托上层父加载器加载，如果所有父加载器在自己的加载类路径下都找不到目标类，则在自己的类加载路径中查找并载入目标类。<br>比如我们的Math类，最先会找应用程序类加载器加载，应用程序类加载器会先委托扩展类加载器加载，扩展类加载器再委托引导类加载器，顶层引导类加载器在自己的类加载路径里找了半天没找到Math类，则向下退回加载Math类的请求，扩展类加载器收到回复就自己加载，在自己的类加载路径里找了半天也没找到Math类，又向下退回Math类的加载请求给应用程序类加载器，应用程序类加载器于是在自己的类加载路径里找Math类，结果找到了就自己加载了。。<br>双亲委派机制说简单点就是，先找父亲加载，不行再由儿子自己加载</p><h3 id="3-4双亲委派机制的好处"><a href="#3-4双亲委派机制的好处" class="headerlink" title="3.4双亲委派机制的好处"></a>3.4双亲委派机制的好处</h3><ul><li>沙箱安全机制：自己写的java.lang.String.class类不会被加载，这样便可以防止核心API库被随意篡改</li><li>避免类的重复加载：当父亲已经加载了该类时，就没有必要子ClassLoader再加载一次，保证被加载类的唯一性</li></ul><h3 id="3-5类加载源码分析"><a href="#3-5类加载源码分析" class="headerlink" title="3.5类加载源码分析"></a>3.5类加载源码分析</h3><p>我们来看下应用程序类加载器AppClassLoader加载类的双亲委派机制源码，AppClassLoader的loadClass方法最终会调用其父类ClassLoader的loadClass方法，该方法的大体逻辑如下：<br>首先，检查一下指定名称的类是否已经加载过，如果加载过了，就不需要再加载，直接返回。<br>如果此类没有加载过，那么，再判断一下是否有父加载器；如果有父加载器，则由父加载器加载（即调用parent.loadClass(name, false);）.或者是调用bootstrap类加载器来加载。<br>如果父加载器及bootstrap类加载器都没有找到指定的类，那么调用当前类加载器的findClass方法来完成类加载。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">//ClassLoader的loadClass方法，里面实现了双亲委派机制</span><br><span class="line">protected Class&lt;?&gt; loadClass(String name, boolean resolve)</span><br><span class="line">    throws ClassNotFoundException</span><br><span class="line">&#123;</span><br><span class="line">    synchronized (getClassLoadingLock(name)) &#123;</span><br><span class="line">        // 检查当前类加载器是否已经加载了该类</span><br><span class="line">        Class&lt;?&gt; c = findLoadedClass(name);</span><br><span class="line">        if (c == null) &#123;</span><br><span class="line">            long t0 = System.nanoTime();</span><br><span class="line">            try &#123;</span><br><span class="line">                if (parent != null) &#123;  //如果当前加载器父加载器不为空则委托父加载器加载该类</span><br><span class="line">                    c = parent.loadClass(name, false);</span><br><span class="line">                &#125; else &#123;  //如果当前加载器父加载器为空则委托引导类加载器加载该类</span><br><span class="line">                    c = findBootstrapClassOrNull(name);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; catch (ClassNotFoundException e) &#123;</span><br><span class="line">                // ClassNotFoundException thrown if class not found</span><br><span class="line">                // from the non-null parent class loader</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            if (c == null) &#123;</span><br><span class="line">                // If still not found, then invoke findClass in order</span><br><span class="line">                // to find the class.</span><br><span class="line">                long t1 = System.nanoTime();</span><br><span class="line">                //都会调用URLClassLoader的findClass方法在加载器的类路径里查找并加载该类</span><br><span class="line">                c = findClass(name);</span><br><span class="line"></span><br><span class="line">                // this is the defining class loader; record the stats</span><br><span class="line">                sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0);</span><br><span class="line">                sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1);</span><br><span class="line">                sun.misc.PerfCounter.getFindClasses().increment();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        if (resolve) &#123;  //不会执行</span><br><span class="line">            resolveClass(c);</span><br><span class="line">        &#125;</span><br><span class="line">        return c;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-6全盘负责委托机制"><a href="#3-6全盘负责委托机制" class="headerlink" title="3.6全盘负责委托机制"></a>3.6全盘负责委托机制</h3><p>“全盘负责”是指当一个ClassLoder装载一个类时，除非显示的使用另外一个ClassLoder，该类所依赖及引用的类也由这个ClassLoder载入。</p>]]></content>
    
    
    <summary type="html">&lt;hr&gt;
&lt;h1 id=&quot;1-1类加载机制&quot;&gt;&lt;a href=&quot;#1-1类加载机制&quot; class=&quot;headerlink&quot; title=&quot;1.1类加载机制&quot;&gt;&lt;/a&gt;1.1类加载机制&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;类加载步骤：加载 &amp;gt; &amp;gt; 验证 &amp;gt; &amp;gt; 准备 &amp;gt; &amp;gt; 解析 &amp;gt; &amp;gt; 初始化 &amp;gt; &amp;gt; 使用 &amp;gt; &amp;gt; 卸载&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    
    <category term="JVM" scheme="https://aj-web.github.io/tags/JVM/"/>
    
  </entry>
  
  <entry>
    <title>JVM-1:内存模型深度剖析与优化</title>
    <link href="https://aj-web.github.io/JVM%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E6%B7%B1%E5%BA%A6%E5%89%96%E6%9E%90%E4%B8%8E%E4%BC%98%E5%8C%96/"/>
    <id>https://aj-web.github.io/JVM%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E6%B7%B1%E5%BA%A6%E5%89%96%E6%9E%90%E4%B8%8E%E4%BC%98%E5%8C%96/</id>
    <published>2021-11-09T16:00:00.000Z</published>
    <updated>2021-12-29T07:25:44.876Z</updated>
    
    <content type="html"><![CDATA[<hr><blockquote><p>前言：为什么要学习JVM？<br>一门语言有可能会过时，但是它的思想是不会过时的，尤其是作为JAVA跨平台的核心实现，JVM的思想值每个程序员得学习</p></blockquote><span id="more"></span><p>Java Virtual Machine(JVM) 是一种抽象的计算机，基于堆栈架构，它有自己的指令集和内存管理。它加载 class 文件，分析、解释并执行字节码。基本结构如下：<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/JVM%E6%9E%84%E6%88%90.png" alt="JVM结构"><br>JVM 主要分为以上三个子系统：类加载器、运行时数据区和执行引擎，下面我们分部分展开理解</p><h1 id="1-JVM之运行时数据区"><a href="#1-JVM之运行时数据区" class="headerlink" title="1.JVM之运行时数据区"></a>1.JVM之运行时数据区</h1><h3 id="1-1-运行时数据区组成"><a href="#1-1-运行时数据区组成" class="headerlink" title="1.1 运行时数据区组成"></a>1.1 运行时数据区组成</h3><p>它约定了在运行时程序代码的数据比如变量、参数等等的存储位置，主要包含以下几部分：   </p><ol><li>程序计数器：也是每个线程独有的，就是代码执行的位置，为什么要设计程序计数器，是为了多线程代码挂起后恢复执行</li><li>本地方法栈：与 JVM 栈类似，只不过服务于 Native 方法   </li><li>堆的组成：存储类实例对象和数组对象，垃圾回收的主要区域</li><li>栈的组成：new出来对象的引用(对象引用) ,基本数据类型和局部变量</li><li>方法区组成：又叫(元空间)，存放运行时常量池，字段和方法的数据，构造函数和方法的字节码等，在 JDK 8 中，把 interned String 和类静态变量移动到了 Java 堆</li></ol><h3 id="1-2-堆栈原理"><a href="#1-2-堆栈原理" class="headerlink" title="1.2 堆栈原理"></a>1.2 堆栈原理</h3><p>讲了其中方法区，程序计数器，本地方法栈比较简单，下面深入理解下，堆和栈的概念：<br>(1)堆由年轻代，老年代组成，年轻带占整个堆的1/3，老年代占整个堆的2/3，配比可以调整，年轻代有Eden区Survivor区，配比为8：1：1，new出来的对象一般在Eden区，Eden放满的时候会执行minor gc，回收无用的对象。<br>回收基本原理，从gcroot，找局部变量 静态变量引用了其他的话，就不是垃圾对象，复制到s0，分代年龄会加1，第二次Eden满的时候s0和Eden都会minor gc存活下来的对象存放到s1，分代年龄+1，当分代年龄到15，会被挪到老年代，老年代满的时候，会full gc，回收后还是满的话，就会内存溢出报错<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/%E5%A0%86%E7%9A%84%E7%BB%84%E6%88%90.png" alt="堆的组成"></p><p>(2)栈是服务于方法的，当启动一个线程的时候，就会在栈中预先分配出一块空间，当线程执行方法的时候，会在这个预先分配的栈空间中创建一个<br>栈帧的数据结构。栈帧(Stack Frame)是用于支持虚拟机进行方法调用和方法执行的数据结构，是用来存储数据和部分过程结果的数据结构，同时也用来处理动态连接、方法返回值和异常分派。<br>栈帧随着方法调用而创建，随着方法结束而销毁——无论方法正常完成还是异常完成都算作方法结束栈帧由以下部分组成：  </p><ul><li>局部变量表：方法的局部变量和方法参数。main方法的局部变量表中对象变量存放的是堆的地址   </li><li>操作数栈：局部变量的操作数的临时的内存空间   </li><li>动态链接：一个指向运行时常量池的引用，将 class 文件中的符号引用（描述一个方法调用了其他方法或访问成员变量）转为直接引用。符号引用一部分会在类加载阶段或者第一次使用时就直接转化为直接引用，这类转化称为静态解析。另一部分将在每次运行期间转化为直接引用，这类转化称为动态连接。  </li><li>方法返回：方法正常退出或抛出异常退出，返回方法被调用的位置<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/%E6%A0%88%E5%B8%A7.png" alt="栈帧"></li></ul><h3 id="1-3-JVM概览"><a href="#1-3-JVM概览" class="headerlink" title="1.3 JVM概览"></a>1.3 JVM概览</h3><p><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/JVM.png" alt="JVM概览">  </p><h1 id="2-JVM内存参数设置"><a href="#2-JVM内存参数设置" class="headerlink" title="2.JVM内存参数设置"></a>2.JVM内存参数设置</h1><p><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/JVM%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE.png" alt="JVM总体参数">     </p><ul><li><p>springboot的jvm参数设置格式(Tomcat启动直接加在bin目录下catalina.sh文件里)：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">java -Xms2048M -Xmx2048M -Xmn1024M -Xss512K -XX:MetaspaceSize=256M -XX:MaxMetaspaceSize=256M -jar microservice-eureka-server.jar</span><br><span class="line">-Xss：每个线程的栈大小</span><br></pre></td></tr></table></figure></li><li><p>关于元空间的JVM参数有两个：<br>-XX:MetaspaceSize=N和<br>-XX:MaxMetaspaceSize=N  </p></li><li><p>XX：MaxMetaspaceSize： 设置元空间最大值， 默认是-1， 即不限制， 或者说只受限于本地内存大小   </p></li><li><p>XX：MetaspaceSize： 指定元空间触发Fullgc的初始阈值(元空间无固定初始大小)， 以字节为单位，默认是21M左右，达到该值就会触发full gc进行类型卸载， 同时收集器会对该值进行调整： 如果释放了大量的空间， 就适当降低该值； 如果释放了很少的空间， 那么在不超过-XX：MaxMetaspaceSize（如果设置了的话） 的情况下， 适当提高该值。这个跟早期jdk版本的</p></li><li><p>XX:PermSize参数意思不一样，<br>-XX:PermSize代表永久代的初始容量。<br>由于调整元空间的大小需要Full GC，这是非常昂贵的操作，如果应用在启动的时候发生大量Full GC，通常都是由于永久代或元空间发生<br>了大小调整，基于这种情况，一般建议在JVM参数中将MetaspaceSize和MaxMetaspaceSize设置成一样的值，并设置得比初始值要大，<br>对于8G物理内存的机器来说，一般我会将这两个值都设置为256M。</p></li></ul><p>结论：<br>-Xss设置越小count值越小，说明一个线程栈里能分配的栈帧就越少，但是对JVM整体来说能开启的线程数会更多</p><h3 id="2-1-日均百万级订单交易系统如何设置JVM参数"><a href="#2-1-日均百万级订单交易系统如何设置JVM参数" class="headerlink" title="2.1 日均百万级订单交易系统如何设置JVM参数"></a>2.1 日均百万级订单交易系统如何设置JVM参数</h3><p><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/%E4%BA%BF%E7%BA%A7%E6%B5%81%E9%87%8FJVM%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE.png" alt="亿级流量JVM参数调优"><br>&nbsp;结论：通过上面这些内容介绍，大家应该对JVM优化有些概念了，就是尽可能让对象都在新生代里分配和回收，尽量别让太多对象频繁进入老年代，避免频繁对老年代进行垃圾回收，同时给系统充足的内存大小，避免新生代频繁的进行垃圾回收。</p>]]></content>
    
    
    <summary type="html">&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;前言：为什么要学习JVM？&lt;br&gt;一门语言有可能会过时，但是它的思想是不会过时的，尤其是作为JAVA跨平台的核心实现，JVM的思想值每个程序员得学习&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    
    <category term="JVM" scheme="https://aj-web.github.io/tags/JVM/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统-4：推荐算法实现与调优</title>
    <link href="https://aj-web.github.io/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%B0%83%E4%BC%98/"/>
    <id>https://aj-web.github.io/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%B0%83%E4%BC%98/</id>
    <published>2021-10-31T16:00:00.000Z</published>
    <updated>2021-12-29T06:18:55.183Z</updated>
    
    <content type="html"><![CDATA[<hr><blockquote><p>经过前面对机器学习算法的了解和对推荐系统理论基础的了解，本节来尝试实现推荐系统</p></blockquote><span id="more"></span><h1 id="1-推荐系统简介"><a href="#1-推荐系统简介" class="headerlink" title="1.推荐系统简介"></a>1.推荐系统简介</h1><p>&emsp;&emsp;推荐系统是利⽤电⼦商务⽹站向客户提供商品信息和建议，帮助⽤户决定应该 购买什么产品，模拟销售⼈员帮助客户完成购买过程。 个性化推荐是根据⽤户的兴 趣特点和购买⾏为，向⽤户推荐⽤户感兴趣的信息和商品。</p><h1 id="2-通用推荐系统模型"><a href="#2-通用推荐系统模型" class="headerlink" title="2.通用推荐系统模型"></a>2.通用推荐系统模型</h1><p>&emsp;&emsp;通用推荐系统有3个重要的模块：⽤户建模模块、推荐对象建模模块、推荐算法模 块。通⽤的推荐系统模型流程如图。推荐系统把⽤户模型中兴趣需求信息和推荐对 象模型中的特征信息匹配，同时使⽤相应的推荐算法进⾏计算筛选，找到⽤户可能 感兴趣的推荐对象，然后推荐给⽤户。</p><h1 id="3-如何实现自己的推荐系统"><a href="#3-如何实现自己的推荐系统" class="headerlink" title="3.如何实现自己的推荐系统"></a>3.如何实现自己的推荐系统</h1><p>（1）推荐客户购买的物品的周边产品？<br>（2）在订单表中找销售量最靠前的产品？<br>对于推荐系统，有⼀个⾮常重要的指标就是推荐产品的覆盖率，也就是推荐出来的产品应该要越丰富越好。这是为什么呢？这就涉及到了电商的⼀个根本性的理论模型-“⻓尾经济模型”。商品的交易⾏为，通常都会遵循⼀个普遍性的2-8理论，即80%的利润出⾃于20%的商品。⽐如我们去超市购物，通常也 都⽐较喜欢购买最热⻔的，品牌印象⼒⼤的商品。所以当我们以产品为X轴，产品带来的利润为Y轴，经过整理通常都能得到⼀个这样的正态分布图<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/20210924093413.png"></p><p>&emsp;&emsp;那其实推荐系统的真正核⼼可以理解为⼀个矩阵求解的数学问题。⽐如，⽹站向⽤户推荐商品，往往要基于⽤户以往的浏览记录或者评价记录，⽽这些记录就可以抽象为（userid，productid，score）这样的⼀个向量结构，score可以是⼀个任意的数字，⽐如在这⾥表示⽤户的浏览次数，也可以是⼀个0或1的值<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/%E7%94%A8%E6%88%B7%E5%95%86%E5%93%81%E5%90%91%E9%87%8F%E7%9F%A9%E9%98%B5.png" alt="推荐模型本体矩阵"><br>&emsp;&emsp;⼀个向量数据,就代表了矩阵中的⼀个点，在这个矩阵中，数据通常是⽐较稀疏的，称为稀疏矩阵，⽽推荐算法要做的，就是将这些矩阵中的空⽩点，以某⼀种⽅法进⾏部分填充或者全部填充</p><h1 id="4-机器学习流程回顾"><a href="#4-机器学习流程回顾" class="headerlink" title="4.机器学习流程回顾"></a>4.机器学习流程回顾</h1><p>###（1）数据收集：<br>&emsp;&emsp;机器学习会通过学习历史数据，总结出⼀些最有可能的规律。当这些规律 达到⼀个⽐较⾼的可信度时，就可以⽤来对未来数据进⾏预测了。所以数据的体量以及质量，往往就决定了机器学习所能达到的⾼度。这也是为什么很多好的机器学习产品最先都是出在像⾕歌、百度这样的⼤公司的，就是因为他们的数据往往是最⼤最全的。<br>&emsp;&emsp;在实际业务中，这些有⽤的数据集成本巨⼤，甚⾄可能包含了很多核⼼的 商业机密。那在学习阶段，我们要怎么去 获得有价值的数据集呢？主要还是通 过直接使⽤别⼈维护好的数据集。<br>(1).UCI： <a href="http://archive.ics.uci.edu/ml/">http://archive.ics.uci.edu/ml/</a> 这个⽹站上维护了很多经典的数据集。<br>(2).kaggle： <a href="https://www.kaggle.com/">https://www.kaggle.com/</a> ⼀个综合性的机器学习竞赛平台。上⾯会开放 很多数据集，开展很多机器学习的竞赛。有很多都是⼀些公司⾃⼰处理不了的实际 数据，数据集的质量通常都是⽐较⾼的。同时也有很多别⼈分享 的基础教程以及算 法分享，也都是⾮常不错的学习资料<br>(1).例如python的sklearn框架就集成了⼀部分常⽤的数据集</p><p>###（2）数据清洗<br>&emsp;&emsp;通过数据清洗，将明显⽆⽤的信息去掉，并且把数据整理成能够被机器学习接收的数据格式。这个过程通常没有固定的⼯作⽅式，需要根据不同的算法不同的要求，指定不同的处理⽅式。数据清洗是前期⼯作量⾮常⼤的⼀个环节，同时也是⾮常考验程序员⼯程能⼒的环节</p><p>###（3）特征工程<br>&emsp;&emsp;筛选出显著特征、摒弃⾮显著特征，需要机器学习⼯程师反复理解业务。这对很多结果有决定性的影响。特征选择好了，⾮常简单的算法也能得出良好、稳定的结果。</p><p>###（4）训练模型：<br>&emsp;&emsp;有了数据之后，你只需要选择⼀个合适的机器学习算法，把数据交给他学习，⾃然就会形成⼀个数据模型。这个过程往往不需要⼈⼯进⾏⼲预。甚⾄很多时候，机器学习到底学习到了哪些规律，⼈也是很难弄明⽩的。在这个过程中，需要注意的是，针对同⼀个问题，往往可以选择很多的算法，甚⾄针对同⼀个算法，也会需要制定不同的超参数。这些组合都会计算出不同的数据模型。所有这些模型都是可选的结果。</p><p>&emsp;&emsp;机器学习算法分类<br>机器学习涉及到⾮常多的数学算法。对这些数学算法，进⾏简单分类。<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%88%86%E7%B1%BB.png" alt="机器学习算法分类"></p><p>###（5）模型优化：<br>&emsp;&emsp;训练出了模型，并不能代表机器学习成功。有了众多的数据模型之后，就需要在这些模型中找出针对当前问题的最佳⽅案。这个优化过程即需要基于对 算法的深⼊了解，同时也需要基于⼤量的尝试。也是⾮常考验算法⼯程师技术的地⽅。<br>&emsp;&emsp;最常⽤的检测⽅案是将整个数据集随机拆分成训练集和测试集。⽤机器学习算法在训练集上学习并形成数据模型，然后拿这个数据模型对测试集的数据进⾏预测，接下来拿预测出来的⽬标值与测试集上实际的⽬标值进⾏⽐对。⽬标值真实结果匹配度最⾼的模型就认为是最好的模型。 另外，从历史数据中学习形成的数据模型，最终还是要回归于对未来业务的指导，⽽未来业务会形成新的数据集。这个时候模型优化⼀个很重要的过程就是需要让模型继续 学习更多新的业务数据，及时优化。这样才能让模型的准确地更⾼。</p><h1 id="5-推荐系统核心技术"><a href="#5-推荐系统核心技术" class="headerlink" title="5.推荐系统核心技术"></a>5.推荐系统核心技术</h1><p>###（1）数据处理<br>&emsp;&emsp;关于推荐系统的数据要求，在之前已经简单做过介绍。推荐学习算法需要的数据是以(userId,ProductId,Rating)这样的向量数据的格式组成的⼀个稀疏矩 阵。⽽userId，productId，这两个关键属性在机器学习算法中，只是作为⼀个与具体对象特征⽆关的⼀个代表值。所以，往往在对推荐系统进⾏业务优化的过程中，会将这个userId和 productId替换成对⽤户特征有⼀定描述性的特征值。也就是建立用户画像。</p><p>&emsp;&emsp;用户画像：用户画像就是与该用户相关联的数据的可视化的展现；一句话来总结就是：用户信息标签化。<br>&emsp;&emsp;标签：标签是通过对用户信息分析而来的高度精炼的特征标识。<br>通过打标签可以利用一些高度概括、容易理解的特征来描述用户，可以让人更容易理解用户，并且可以方便计算机处理。<br>&emsp;&emsp;用户画像的数据来源主要包括两个方面：<br>&emsp;&emsp;属性数据，这部分数据一般是用户的注册信息，也可以是从其他数据中分析得出的。比如生日、性别、住址、爱好等<br>&emsp;&emsp;行为数据，这部分数据一般都是用户的访问日志记录的行为数据。比如常用的一些后端日志数据、前端埋点数据等等。</p><p>###（2）推荐算法</p><ul><li><p>1.协同过滤算法(常用)</p><p>&emsp;&emsp;协同过滤(Collaborative Filtering Recommendation)，协同过滤有以下两个变种<br>(1) 基于用户的协同过滤推荐 (User-based Collaborative Filtering Recommendation)基于用户的协同过滤推荐算法先使用统计技术寻找与目标用户有相同喜好的邻居，然后根据目标用户的邻居的喜好产生向目标用户的推荐。基本原理就是利用用户访问行为的相似性来互相推荐用户可能感兴趣的资源，如图所示：<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/202109241617260.png" alt="用户过滤"><br>(2) 基于物品的协同过滤推荐 (Item-based Collaborative Filtering Recommendation)根据所有用户对物品或者信息的评价，发现物品和物品之间的相似度，然后根据用户的历史偏好信息将类似的物品推荐给该用户，如图所示：<br>喜欢物品 A 的都喜欢物品 C，基于这个判断用户 C 可能也喜欢物品 C，所以推荐系统将物品 C 推荐给用户 C<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/202109241621964.png" alt="物品过滤"></p></li></ul><p>&emsp;&emsp;协同过滤的基本原理：根据用户的历史行为数据计算用户或者物品的相似度(比如使用余弦相似度)，然后进行推荐。<br>举个例子：首先我们根据网站的记录计算出一个用户与item的关联矩阵，如下：<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/202109241622399.png"><br>&emsp;&emsp;那么(x, y)的值则是y用户对x物品的评分（喜好程度）。我们可以把每一行视为一个用户对物品偏好的向量，然后计算每两个用户之间的向量距离，这里我们用余弦相似度来算：<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/202109241623694.png"><br>&emsp;&emsp;计算后得到结果如下<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E8%AE%A1%E7%AE%97%E7%BB%93%E6%9E%9C.png"><br>&emsp;&emsp;可以看到针对同一个物品，算出来的值，两个用户的值越接近，他们也就越相似，那么相似的用户就可以互相推荐了<br>基于物品的CF计算方式大致相同，只是关联矩阵变为了item和item之间的关系，若用户同时浏览过item1和item2，则(1,1)的值为1。<br>述基于近邻的算法（K-NN）是非常简单且流行的。</p><p>协同过滤</p><ul><li>2.基于流行度的算法<br>&emsp;&emsp;基于流行度的算法非常简单粗暴，类似于各大新闻、微博热榜等，根据PV、UV、日均PV或分享率等数据来按某种热度排序来推荐给用户<br>&emsp;&emsp;这种算法的优点是简单，适用于刚注册的新用户。缺点也很明显，它无法针对用户提供个性化的推荐。基于这种算法也可做一些优化，比如加入用户分群的流行度排序，例如把热榜上的体育内容优先推荐给体育迷，把政要热文推给热爱谈论政治的用户。</li><li>3.基于内容的算法(常用)<br>&emsp;&emsp;CF算法看起来很好很强大，通过改进也能克服各种缺点。那么问题来了，假如我是个《指环王》的忠实读者，我买过一本《双塔奇兵》，这时库里新进了第三部：《王者归来》，那么显然我会很感兴趣。然而基于之前的算法，无论是用户评分还是书名的检索都不太好使，于是基于内容的推荐算法呼之欲出。</li></ul><p>内容推荐：<br>&emsp;&emsp;举个例子，现在系统里有一个用户和一条新闻。通过分析用户的行为以及新闻的文本内容，我们提取出数个关键字，如下图：<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/202109261703197.png"></p><p>内容推荐是为了更加匹配的更加精准，往往会进行一下的优化：<br>举个例子：<br>&emsp;&emsp;如果在为一名热爱观看英超联赛的球迷推荐新闻时，新闻里同时存在关键字体育、足球、英超，显然匹配前两个词都不如直接匹配英超来得准确，系统该如何体现出关键词的这种“重要性”呢？这时我们便可以引入词权的概念。在大量的语料库中通过计算（比如典型的TF-IDF算法），我们可以算出新闻中每一个关键词的权重，在计算相似度时引入这个权重的影响，就可以达到更精确的效果。<br>&emsp;&emsp;但是用户的兴趣是足球，而新闻的关键词是德甲、英超，按照上面的文本匹配方法显然无法将他们关联到一起。在此，我们可以引用话题聚类：<br>&emsp;&emsp;利用word2vec一类工具，可以将文本的关键词聚类，然后根据topic将文本向量化。如可以将德甲、英超、西甲聚类到“足球”的topic下，将lv、Gucci聚类到“奢侈品”topic下，再根据topic为文本内容与用户作相似度计算。</p><ul><li>4.基于模型的算法(Model-based Collaborative Filtering Recommendation)<br>基于模型的协同过滤推荐就是基于样本的用户喜好信息，训练一个推荐模型，然后根据实时的用户喜好的信息进行预测推荐。<br>以美团构建的一个模型来举例子<br><a href="https://tech.meituan.com/2015/01/22/mt-recommend-practice.html">https://tech.meituan.com/2015/01/22/mt-recommend-practice.html</a><br>location-based：<br>&emsp;&emsp;对于移动设备而言，与PC端最大的区别之一是移动设备的位置是经常发生变化的。不同的地理位置反映了不同的用户场景，在具体的业务中可以充分利用用户所处的地理位置。在推荐的候选集触发中，我们也会根据用户的实时地理位置、工作地、居住地等地理位置触发相应的策略。</li><li>5.混合算法<br>&emsp;&emsp;由于各种推荐方法都有优缺点，所以在实际中，组合推荐（Hybrid Recommendation）经常被采用。研究和应用最多的是内容推荐和协同过滤推荐的组合。最简单的做法就是分别用基于内容的方法和协同过滤推荐方法 去产生一个推荐预测结果，然后用某方法组合其结果。尽管从理论上有很多种推荐组合方法，但在某一具体问题中并不见得都有效，组合推荐一个最重要原则就是通过组合后要能避免或弥补各自推荐技术的弱点<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/%E6%B7%B7%E5%90%88%E6%8E%A8%E8%8D%90.png"></li><li>6.隐语意模型<br>&emsp;&emsp;在常见的协同过滤中，我们要求只有在两个用户对同一个物品进行评分或者两个物品被同一个用户打分后我们才可以计算用户的相似度。这种计算相似度的方式其实没有考量到物品或者用户背后的关联性。所以我们通过把物品和用户都拆解成隐语意向量的方式，来对用户对物品的评分进行预测，这就是隐语意模型的意义。<br>交替最⼩⼆乘法( Alternating Least Squares , ALS )<br>&emsp;&emsp;ALS的思想是，由于两个矩阵P和Q都未知，且通过矩阵乘法耦合在⼀起，为了使它们解耦，可以先固定Q，把P当作变量，通过损失函数最⼩化求出P,这就是-个经典的最⼩⼆乘问题;再反过来固定求得的P,把Q当作变量，求解出Q:如此交替执⾏，直到误差满⾜阈值条件，或者到达迭代上限</li><li>7.损失函数<br>&emsp;&emsp;矩阵分解得到的预测评分矩阵R’,与原评分矩阵R在已知的评分项.上可能会有误差，  这个时候需要构建合适的损失函数来进行评估，损失函数有很多种，在这先不细讲。</li></ul><h1 id="6-基于常识的评判标准"><a href="#6-基于常识的评判标准" class="headerlink" title="6.基于常识的评判标准"></a>6.基于常识的评判标准</h1><p>&emsp;&emsp;从上⾯我们已经知道了，推荐系统其实只是⼀个数字游戏，但是我们的业务不可能是简单的数字游戏。有⼀些推荐的结果，我们是可以直接从业务上判断是好是坏的，其中坏的推荐的⽐例，就可以作为对推荐系统⼀个评判的标 准。    ⽐如，对于天猫，淘宝这类电商⽹站，给⽤户推荐他已经购买过的商 品，往往就不是⼀个⽐较好的结果。但是推荐已经购买过的商品的周边商品，  这个结果就⽐较好。就⽐如⽤户购买了⼀个⿏标，⽤户再去购买⿏标的概率就⽐较低，但是如果推荐⼀个配套的⿏标垫，⿏标贴，⽤户去购买的可能性就会⽐较⾼。<br>&emsp;&emsp;对于⼀个⾳乐类的内容推荐系统，如果⽤户是某⼀个歌星的粉丝，那么再 去给他推荐这个歌星的其他的歌或者专辑，意义就不是那么⼤，因为这些内容⽤户通常都会主动搜索。再⽐如对⼀个新闻类的内容推荐系统，如果推荐给⽤ 户的新闻包含了很多”过时”的内容，或者推荐很久之前的帖⼦，那么显然也不 是⼀个好的推荐</p><p>基于指标的评判标准<br>&emsp;&emsp;通常对于业务系统的评价，还是会回归到业务的本身。所以对于推荐系统 最常⻅的评判标准，还是通过⼀定的业务指标来衡量。例如常⻅的PV、UV、⽤ 户留存率、转化率等等，通过⽐对上推荐系统之前和之后的指标数据，来衡量<br>⼀个推荐系统是不是有效，或者拿推荐系统优化前后的数据进⾏⽐较，来看推 荐系统的优化是否有效。⽐如对于猫眼这样的购票系统，最直接的衡量标准就 是推荐系统带来的流量和收⼊的增⻓</p><p>基于机器学习的评判标准<br>&emsp;&emsp;通常推荐系统需要结合⼤量的业务数据，通过对历史数据的挖掘、分析，  归纳出⽤⼾与产品之间的⼀些关系。这些 关系通常过于隐晦，有些是能够进⾏ 解释的特征，⽐如⽤⼾的爱好、产品的受众特点等。但是往往还有很多隐藏的 关系是⽆法⽤简单的常理来解释的。这些关系就要通过机器学习的算法来进⾏ 深⼊挖掘。最终通过这些分析，所以现在业界普遍的推荐系统都是基于机器学 习算法来完成的。⽽每⼀个机器学习的算法，都会有他⾃⼰的评判指标和优化<br>⽅式</p>]]></content>
    
    
    <summary type="html">&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;经过前面对机器学习算法的了解和对推荐系统理论基础的了解，本节来尝试实现推荐系统&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    
    <category term="机器学习" scheme="https://aj-web.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="推荐系统" scheme="https://aj-web.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统-3：特征工程基础</title>
    <link href="https://aj-web.github.io/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/"/>
    <id>https://aj-web.github.io/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E5%9F%BA%E7%A1%80/</id>
    <published>2021-10-29T16:00:00.000Z</published>
    <updated>2021-12-29T06:17:07.638Z</updated>
    
    <content type="html"><![CDATA[<hr><blockquote><p>在进行了数据收集，数据清洗，就要开始特征工程了，数据和特征决定了机器学习的上限，而模型和算法只是不断逼近这个上限而已，所以特征工程是非常重要的。这一节我们来学习特征工程。</p></blockquote><span id="more"></span><h1 id="1-机器学习目标"><a href="#1-机器学习目标" class="headerlink" title="1.机器学习目标"></a>1.机器学习目标</h1><p>&emsp;&emsp;在机器学习中，最为典型的分类算法和回归算法，他们的处理流程也是类似于这样一个过程。通过对历史数据的推断，寻找数字之间的规律，从而预测出后面的数字是什么样的。只不过，他要处理的问题，比这个简单的数字推断更复杂。机器学习的历史数据不再是一个一个的数字，而是由多个数字组成的向量。并且，数据之间的规律更难找到，同时也没有这么稳固。很可能数字之间并没有完全准确的规则，这时就需要选择出一个相对靠谱的数字来。其实这个思想跟之前的数字推断是一样的。所以总结来说机器学习处理的是数字向量之间的问题。</p><h1 id="2-机器学习标准处理流程"><a href="#2-机器学习标准处理流程" class="headerlink" title="2.机器学习标准处理流程"></a>2.机器学习标准处理流程</h1><ul><li><p>数据收集：<br>&emsp;&emsp;机器学习会通过学习历史数据，总结出一些最有可能的规律。当这些规律达到一个比较高的可信度时，就可以用来对未来数据进行预测了。所以数据的体量以及质量，往往就决定了机器学习所能达到的高度。这也是为什么很多好的机器学习产品最先都是出在像谷歌、百度这样的大公司的，就是因为他们的数据往往是最大最全的。</p></li><li><p>2.数据清洗：<br>&emsp;&emsp;有了数据之后还需要进行清洗。原始的数据就像是矿石，往往含金量非常低。这个时候就要通过数据清洗，将明显无用的信息去掉，并且把数据整理成能够被机器学习接收的数据格式。这个过程通常没有固定的工作方式，需要根据不同的算法不同的要求，指定不同的处理方式。数据清洗是前期工作量非常大的一个环节，同时也是非常考验程序员工程能力的环节</p></li><li><p>3.训练模型：<br>&emsp;&emsp;这个过程是最关键的，但是其实他也是比较简单的。有了数据之后，你只需要选择一个合适的机器学习算法，把数据交给他学习，自然就会形成一个数据模型。这个过程往往不需要人工进行干预。甚至很多时候，机器学习到底学习到了哪些规律，人也是很难弄明白的。在这个过程中，需要注意的是，针对同一个问题，往往可以选择很多的算法，甚至针对同一个算法，也会需要制定不同的超参数。这些组合都会计算出不同的数据模型。所有这些模型都是可选的结果</p></li><li><p>4.模型优化：<br>&emsp;&emsp;训练出了模型，并不能代表机器学习成功。有了众多的数据模型之后，就需要在这些模型中找出针对当前问题的最佳方案。这个优化过程即需要基于对算法的深入了解，同时也需要基于大量的尝试。也是非常考验算法工程师技术的地方。<br>&emsp;&emsp;最常用的检测方案是将整个数据集随机拆分成训练集和测试集。用机器学习算法在训练集上学习并形成数据模型，然后拿这个数据模型对测试集的数据进行预测，接下来拿预测出来的目标值与测试集上实际的目标值进行比对。目标值真实结果匹配度最高的模型就认为是最好的模型。我们经常说的人脸识别准确率达到多少多少，其实就是在测试集上的比对结果。<br>&emsp;&emsp;另外，从历史数据中学习形成的数据模型，最终还是要回归于对未来业务的指导，而未来业务又会形成新的数据集。这个时候模型优化一个很重要的过程就是需要让模型继续学习更多新的业务数据，及时优化。这样才能让模型的准确地更高。</p></li></ul><h1 id="3-常用的特征工程方法"><a href="#3-常用的特征工程方法" class="headerlink" title="3. 常用的特征工程方法"></a>3. 常用的特征工程方法</h1><h2 id="3-1-特征抽取"><a href="#3-1-特征抽取" class="headerlink" title="3.1 特征抽取:"></a>3.1 特征抽取:</h2><p>&emsp;&emsp;机器学习只能学习数字类型的特征值，但是有些数据集他的原始数据不是数字类型，比如图像、文本、字符等。这时，就需要使用特征抽取将数据转化成适合机器学习的数字特征。</p><h3 id="3-1-1-文本特征抽取："><a href="#3-1-1-文本特征抽取：" class="headerlink" title="3.1.1 文本特征抽取："></a>3.1.1 文本特征抽取：</h3><p>&emsp;&emsp;场景：现在如果我们要对文本的分类情况进行机器学习，这是一个典型的分类问题，但是这个特征值似乎跟我们之前提到的特征值不太一样。文本没有那些属性和数字啊。那应该怎么抽取特征值呢？</p><h3 id="3-1-1-one-hot编码："><a href="#3-1-1-one-hot编码：" class="headerlink" title="3.1.1 one-hot编码："></a>3.1.1 one-hot编码：</h3><p>&emsp;&emsp;对于字典类型的数据特征，例如 性别、城市、颜色这一类字典类型数据，通常在数据库中，会以一个数字编码标识，如 0:男,1:女 这样。但是，如果在机器学习中使用这样的数字编码，就会给学习过程造成误解。因为不同的字典值特性应该是完全“平等”，而如果是 0，1，2这样的数字，则可能给机器学习造成误解，觉得这些字典值是有大小关系的。所以，机器学习中常用的方式是把字典值处理成one-hot编码。<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/one-hot%E7%BC%96%E7%A0%81.png" alt="one-hot编码"></p><h3 id="3-1-2-CountVectorize："><a href="#3-1-2-CountVectorize：" class="headerlink" title="3.1.2 CountVectorize："></a>3.1.2 CountVectorize：</h3><p>&emsp;&emsp;对于文本类型的数据，如一篇文章。在做机器学习时，最基础的处理方式是以文章中的单词出现次数作为特征。处理成 [(word1,count1),(word2,count2)…]这样的格式。这也是mapreduce、spark最经典的入门计算方式。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">缺点：在文章分类等机器学习场景中，体现不出文章的重要特征。例如一般出现次数最多的一些词，如，这里、那里、我们、他们等，并不能体现文章的内容特征。这类词称为停用词。</span><br></pre></td></tr></table></figure><h3 id="3-1-3-TfidfVectorizer："><a href="#3-1-3-TfidfVectorizer：" class="headerlink" title="3.1.3 TfidfVectorizer："></a>3.1.3 TfidfVectorizer：</h3><p>&emsp;&emsp;我们的目的是为了对文本进行分类，但是简单的以单词出现的次数来分类，从经验上判断，会有些问题。长的文章中各个词出现的次数都会比较多，而短的文章各个词普遍都会比较少。这样长的文章对分类结果的影响就会被放大。这时改进的办法就是TF-IDF<br>&emsp;&emsp;TF-IDF可以用来评估一个字词对于一个文件集合或者一个语料库中的其中一份文件的重要程度。例如，在对一大堆文章进行分类时，出现 计算机、软件、云、java这些词的次数比较多的文章更多可能归为科技类(在其他类中出现就比较少，这样的词才有重要性)，而出现 银行、信贷、信用卡 这类词出现次数较多的文章更多可能归为金融类。而所有文章中出现次数都比较多的 我们、你们、这里、那里等这一类的词则对分类来说，意义不大。</p><p>TF-IDF由TF和IDF两部分组成:<br>&emsp;&emsp;TF：词频 term frequency。某一个给定的词语在文章中出现的评率<br>&emsp;&emsp;IDF：逆向文档评率 inverse document frequency.是一个词语普遍重要性的度量。 为总文件数目 除以 包含该词语的文件的数量，再取 10为底对数。<br>&emsp;&emsp;最终TF-IDF=TF*IDF</p><p>示例： 关键词：“经济”；语料库： 1000篇文章；10篇文章出现“经济”。</p><p>TF(经济) = 10/1000 = 0.01 ；IDF(经济)=lg(1000/10)=2</p><p>最终 TF-IDF(经济) = TF(经济)*IDF(经济) = 0.02</p><h2 id="3-2-特征预处理"><a href="#3-2-特征预处理" class="headerlink" title="3.2 特征预处理"></a>3.2 特征预处理</h2><p>&emsp;&emsp;现在我们考虑这样一组特征值： 用户年龄和用户收入。我们能发现，年龄的数字相比收入的数字会小很多。根据之前特征要平等的原则，直觉上就会觉得，这一组数据集中，用户收入的特征会被放大，而用户年龄的特征就容易被忽略。<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/%E7%89%B9%E5%BE%81%E5%80%BC%E5%BD%92%E4%B8%80%E5%8C%96.png" alt="归一化"></p><p>&emsp;&emsp;你可能会想，这组数据，我把用户收入除100，是不是也就把收入固定到了0~100的范围，跟年龄差不多？这确实也是一种处理办法，但是，这样的处理方法，一方面，量纲的影响还是没有完全统一，范围并没有填满。另一方面，用户收入这个特征的很多信息其实就丢失了，拿去机器学习就会丢失很多特征，效果就不会太好。那业界比较常用的方式有两种，归一化 和 标准化。</p><h3 id="3-2-1-归一化"><a href="#3-2-1-归一化" class="headerlink" title="3.2.1 归一化"></a>3.2.1 归一化</h3><p>&emsp;&emsp;归一化通过对原始数据进行变换把数据映射到[0,1]这样一个标准区间。而这个标准区间是可以根据实际情况调整的。<br>他的标准计算公式如下：<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/%E5%BD%92%E4%B8%80%E5%8C%96%E6%96%B9%E7%A8%8B.png" alt="归一化方程"></p><pre><code>其中max,min分别表示这一列特征值中的最大值和最小值。 而mx,mi表示指定的映射区间。默认mx为1，mi为0。归一化的缺点：对异常值敏感。当数据集中出现一个不太合理的极大值或者极小值时，整个归一化的结果就非常不好。鲁棒性(稳定性)较差</code></pre><h3 id="3-2-2-标准化"><a href="#3-2-2-标准化" class="headerlink" title="3.2.2 标准化"></a>3.2.2 标准化</h3><p>&emsp;&emsp;上面提到，归一化对异常值是非常敏感的。而标准化就能很好的处理这种异常数据的问题。<br>&emsp;&emsp;标准化是通过对原始数据进行变换，把数据变换到均值为0，标准差为1的范围内。<br>&emsp;&emsp;他的计算公式是：x’= (x-mean)/std<br>mean： 特征值的均值， std：标准差。 均方差。<br>这种方式，对于极大或极小的少量异常点，均值和标准差都会比较稳定，所以异常值的影响就变小了。</p><h2 id="3-3-降维"><a href="#3-3-降维" class="headerlink" title="3.3 降维"></a>3.3 降维</h2><p>&emsp;&emsp;降维是指在某些限定条件下，降低特征的个数，得到一组”不相关”的主变量的过程。那什么叫”不相关”呢？我们先简单的理解下什么叫特征与特征相关。例如，我们需要去学习某一个地区的降雨量，就会去统计一些常用的天气特征。而这其中，相对湿度与降雨量就是一个相关的特征，相对湿度大，肯定降雨量就会偏大。在进行机器学习训练算法时，如果特征本身存在问题或者特征之间相关性较强，那对于算法学习预测的影响就会比较大。而我们将为的过程，不光是要降低特征值的个数，同时也要尽量去除不相关的特征。</p><h3 id="3-3-1-主特征选择"><a href="#3-3-1-主特征选择" class="headerlink" title="3.3.1 主特征选择"></a>3.3.1 主特征选择</h3><ul><li>方差选择法： 过滤低方差特征(过于集中的数据)  </li><li>相关系数法：特征与特征之间的相关程度。例如 天气湿度 与 降雨量 一般就认为是相关性很强的特征。</li></ul><h3 id="3-3-2-主成分分析"><a href="#3-3-2-主成分分析" class="headerlink" title="3.3.2 主成分分析"></a>3.3.2 主成分分析</h3><ul><li>一种将高维数据转换为低维数据的方法。保留对目标值结果影响较大的特征值，去掉相对影响较小的特征值，例如拍照，三维空间中的人存储在二维的照片中</li><li>简单的理解就是特征个数太多，不好分析时，就可以用PCA来减少特征个数。</li></ul>]]></content>
    
    
    <summary type="html">&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;在进行了数据收集，数据清洗，就要开始特征工程了，数据和特征决定了机器学习的上限，而模型和算法只是不断逼近这个上限而已，所以特征工程是非常重要的。这一节我们来学习特征工程。&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    
    <category term="机器学习" scheme="https://aj-web.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="推荐系统" scheme="https://aj-web.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    <category term="特征工程" scheme="https://aj-web.github.io/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统-2：特征工程基础</title>
    <link href="https://aj-web.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%B8%E5%9E%8B%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/"/>
    <id>https://aj-web.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%B8%E5%9E%8B%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0/</id>
    <published>2021-10-26T16:00:00.000Z</published>
    <updated>2021-12-29T06:18:55.186Z</updated>
    
    <content type="html"><![CDATA[<hr><blockquote><p>机器学习适用的数据为具有特征值(属性，label)和目标值(结果,point)的数据集。通过从历史数据集中学习经验，建立模型，从而达到预测新特征值对应的目标值的效果。因此在数据方面，越见多识广的数据集(样本集越大越全)，越能进行更可信的预测(越准确的预测)。</p></blockquote><span id="more"></span><h1 id="1-KNN算法"><a href="#1-KNN算法" class="headerlink" title="1. KNN算法"></a>1. KNN算法</h1><p>1.<strong>定义：</strong><br>如果一个样本在特征空间中的K个最相似(距离最近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。人以类聚，物以群分。  </p><p>2.<strong>距离计算公式：</strong><br>&emsp;&emsp;欧式距离(平方根距离)、曼哈顿距离(绝对值距离)，明科夫斯基距离(以上两种距离均是明科夫斯基距离的特例)</p><p>3.<strong>适用案例：</strong><br>&emsp;&emsp;iris，根据鸢尾花的一些特征判断一个鸢尾花所属的种类，适用于小数据场景，K-W数据量级别的样本</p><p>4.<strong>算法优缺点：</strong><br>&emsp;&emsp;优点：简单、易于理解，易于实现，无需训练<br>&emsp;&emsp;缺点：1、懒惰算法，对预测样本分类时才进行计算，计算量大，内存开销大。 2、必须指定K值，K值选择会极大程度影响分类的准确度。 关于K值选取：K值过小，容易受到异常数据的影响。而K值过大，容易受样本不均衡的影响。</p><p>5.<strong>特征工程处理：</strong><br>&emsp;&emsp;需尽量保证各个维度的数据公平性。无量纲化-标准化处理。尽量保证各个维度的数据公平性。</p><p>6.<strong>skLearn API:</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sklearn.neighbors.KNeighborsClassifier(n_neighbor=5,algrithm=&#x27;auto&#x27;)</span><br><span class="line">n_neighbors: int 可选，默认5 ， K值</span><br><span class="line">algorithm : &#123;&#x27;auto&#x27;,&#x27;ball_tree&#x27;,&#x27;kd_tree&#x27;,&#x27;brute&#x27;&#125; .可选。用于计算最近的算法。有ball_tree、kd_tree。不同的实现方式会影响效率，但不影响结果。一般用auto，会自己根据fit方法的值来选择合适的算法。</span><br></pre></td></tr></table></figure><h1 id="2-朴素贝叶斯算法"><a href="#2-朴素贝叶斯算法" class="headerlink" title="2. 朴素贝叶斯算法"></a>2. 朴素贝叶斯算法</h1><p><strong>1.朴素：</strong><br>&emsp;&emsp;假定了特征与特征之间相互独立，没有影响。  </p><p><strong>2.朴素的概率计算：</strong><br>&emsp;&emsp;30%的男人是好人，80%的老人是好人， 那男老人有24%的概率是好人。</p><blockquote><p>概率基础知识：独立的定义 P(A,B) = P(A)*P(B)</p><p>全概率公式： P(A)=P(A|B1)*P(B1)+P(A|B2)*P(B2)+P(A|B3)*P(B3)+P(A|B4)*P(B4)+。。。。</p></blockquote><p><strong>3.应用场景：</strong><br>&emsp;&emsp;文本分类、评论区分好差评</p><p><strong>4.优缺点：</strong><br>&emsp;&emsp;优点：发源于古典数学理论，有稳定的分类效率。对缺失数据不敏感，算法也比较简单，常用语文本分类。<br>&emsp;&emsp;分类速度快，准确度相对较高。<br>&emsp;&emsp;缺点：由于使用了样本属性独立的假设，当特征属性之间有关联时，其效果就不太好。<br>&emsp;&emsp;基于概率论的简单统计，样本数量越大越好。</p><p><strong>5.Sklearn API:</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sklearn.naive_bayes.MuitinomialNB(alpha = 1.0)</span><br><span class="line">alpha:拉普拉斯平滑系数，一般用1 。用来防止计算出的概率为0.使用方式是在分子和分母上一起添加平滑系数。</span><br></pre></td></tr></table></figure><h1 id="3-决策树"><a href="#3-决策树" class="headerlink" title="3. 决策树"></a>3. 决策树</h1><p>&emsp;&emsp;决策树是用来解决分类问题的。决策树模型就是一个多层的if-else结构。  </p><p><strong>1.分类原理：</strong><br>&emsp;&emsp;决策树的关键就是判断的关键特征的先后顺序，要能尽量快速过滤掉大部分不符合标准的情况。<br>&emsp;&emsp;决策树的划分依据之一：信息熵 entropy，信息增益。信息熵可认为是信息的混乱程度。而信息增益可以理解为按某一个属性进行划分后的信息熵增益。决策树的划分方式就是在每一个节点选择信息增益最大的属性进行划分。<br>&emsp;&emsp;关于信息熵和信息增益的计算，在数学上有明确的计算公式，但是这里就不去过多推演了。</p><p><strong>2.SkLearn API</strong>  </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">决策树分类器</span><br><span class="line">criterion: 默认是&#x27;gini&#x27;系数，也可以选择信息增益的熵‘entropy’</span><br><span class="line">max_depth:树的深度大小</span><br><span class="line">random_state:随机数种子</span><br></pre></td></tr></table></figure><p>熵的计算比较复杂，默认会使用相对比较简单的gini系数。gini系数可以理解为是信息熵的一个近似结果。</p><p><strong>3.优缺点：</strong><br>&emsp;&emsp;优点：决策树模型的可解释能力强，不需要进行数据归一。模型就是一个多层的If-else结构，比较容易理解。<br>&emsp;&emsp;缺点：每个决策边界只能涉及到一个特征，不太容易扩展到更复杂的情况，整个决策树容易过大。树的深度过大就容易产生过拟合。<br>&emsp;&emsp;过拟合的调整方式是配置maxDepth参数，修改树的深度。改进方法是进行剪枝，防止整个树过于庞大。另一种比较好的方式就是采用随机森林</p><h1 id="4-随机森林"><a href="#4-随机森林" class="headerlink" title="4. 随机森林"></a>4. 随机森林</h1><p>&emsp;&emsp;通过建立几个模型组合来解决单一预测问题。其原理就是生成多个分类器(模型)，各自独立地学习和做出预测。这样预测最后结合成组合预测，一般都优于任何一个单一分类器做出的预测。其输出的类别就是取多数的结果。</p><p><strong>1.原理：</strong><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;BootStrap随机有放回抽样。从训练集中随机取一个样本，放入新训练集。然后从原训练集中再随机抽取(已抽取的样本放回原训练集)。这样，每棵树的训练集都是独有的。相当于将数据分散成不同的树。随机的预测思想就是这些树中有“正确”的树，也有“错误”的树。而“错误”的树的学习结果会互相抵消，最终“正确”的树就会脱颖而出。</p><p><strong>2.sklearn API：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sklearn.ensemble.RandomForestClassifier(n_estimators=10,criterion=&#x27;gini&#x27;,max_depth=None,bootstrap=True,random_state=None,min_samples_split=2)</span><br><span class="line">n_estimators: Int 默认10 森林里的树木数量</span><br><span class="line">criteria： String  默认gini . 分隔特征的测量方法  entropy</span><br><span class="line">max_depth: Integer或者None,可选。树的最大深度</span><br><span class="line">max_features=&quot;auto&quot; 每个决策树的最大特征数量</span><br><span class="line">auto,  sqrt  log2   None</span><br><span class="line">auto和sqrt是一个意思，开根号。 None取跟原样本一样的特征树。</span><br><span class="line">bootstrap: boolean 默认true 是否在构建树时使用放回抽样</span><br><span class="line">min_samples_split:节点划分最少样本数</span><br><span class="line">min_samples_leaf:叶子节点的最小样本数</span><br></pre></td></tr></table></figure><p><strong>3.超参数：</strong><br>n_estimator, max_depth,min_samples_split,min_samples_leaf。</p><p><strong>4.优缺点：</strong><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;在所有分类算法中，具有较好的准确率。能够有效的运行在大数据集上，处理具有高维特征的输入样本，而且不需要降维。能够评估各个特征在分类问题上的重要性。</p><h1 id="5-线性回归"><a href="#5-线性回归" class="headerlink" title="5. 线性回归"></a>5. 线性回归</h1><p>&emsp;&emsp;回归问题：目标值是一组连续的数据。找到一种函数关系，来表示特征值与目标值之间的关系。<br>&emsp;&emsp;而线性回归就是以一个多元一次函数的方式来尽量的逼近所有的目标点。<br>&emsp;&emsp;函数形式：y=f(x)=w_1<em>x_1+w_2</em>x_2+w_3<em>x_3+…..+b。 如果是二维y=w_1</em>x_1+b的话，大家应该非常熟悉，在坐标系上就是一条线，所以叫线性回归。<br>&emsp;&emsp;线性回归问题的关键就是要找出一组最适合的参数值w(形式为一个矩阵)和一个偏移量b。</p><p><strong>1.原理：</strong><br>&emsp;&emsp;用一个多元一次方程来描述特征值与目标值之间的关系，线性关系。</p><p>使用场景：波士顿房价预测Demo<br>目标：<br>模型参数能够尽量准确的预测目标值。–损失函数最小。</p><p><strong>2.SKlearn API：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">--正规方程线性回归</span><br><span class="line">sklearn.linear_model.LinearRegression(fit_interfcept=True)</span><br><span class="line">- 通过正规方程优化</span><br><span class="line">- fit_intercept： 是否计算偏置--参数序列最后的b</span><br><span class="line">- LinearRegression.coef_ ： 回归系数</span><br><span class="line">- LinearRegression.intercept_ ： 偏置</span><br><span class="line"></span><br><span class="line">--随机梯度下降线性回归</span><br><span class="line">sklearn.linear_model.SGDRegressor(loss=&quot;squard_loss&quot;,fit_intercept=True,learning_rate=&#x27;invscaling&#x27;,eta0=0.01)</span><br><span class="line">- SGDRegressor类实现了随机梯度下降学习，支持不同的loss函数和正则化惩罚项来拟合线性回归模型</span><br><span class="line">- loss: 损失类型  squared_loss 普通最小二乘法</span><br><span class="line">- fit_intercept: 是否计算偏置</span><br><span class="line">- learning_rate: string,可选项：</span><br><span class="line">  constant : eta=eta0</span><br><span class="line">  optimal: eta=1.0/(alpha*(t+t0)) --default</span><br><span class="line">  invscaling.: eta = eta0/pow(t,power_t)</span><br><span class="line">  常用constant</span><br><span class="line">  返回结果</span><br><span class="line">  SGDRegressor.coef_ ：回归系数</span><br><span class="line">  SGDRegressor.intercept_ ：偏置</span><br><span class="line"></span><br><span class="line">--计算均方误差</span><br><span class="line">sklearn.metrics.mean_squared_error(y_true,y_predict)</span><br><span class="line">return  浮点数结果</span><br></pre></td></tr></table></figure><p><strong>3.SparkDemo:</strong><br>&emsp;&emsp;JavaLinearSVCExample，计算正规方程。<br>JavaLinearRegressionWithElasticNetExample：包含了误差计算。这个ElasticNet就是线性回归用于优化损失函数的一种方式。</p><p><strong>4.正规方程与线性回归</strong><br>&emsp;&emsp;在线性回归问题中，机器学习的目标就是不断减少损失函数。而优化损失函数的方法有两种：</p><ul><li>一种是像我们计算二元一次方程组一样，直接用公式去计算最佳的一组解。称为正规方程。这种方式不需要学习，直接求解。但是运算在大数据量下的计算会非常复杂，通常只适用于小数据量。<br>另一种是先假定一组解，然后不断试错，慢慢逼近正确答案。称之为梯度下降。<br>几种梯度下降的优化方法：</li></ul><p>&emsp;&emsp;(1)GD： Grandient Descent ; 原始的梯度下降方式。</p><p>&emsp;&emsp;(2)SGD：Stochastic Grandient Descent：随机梯度下降。比较高效，节省时间。缺点是需要许多超参数，对特征标准化敏感。</p><p>&emsp;&emsp;(3)SAG：Stochastic Average Gradient 。 随机平均梯度法。比SGD的收敛速度更快</p><p><strong>5.spark示例：</strong><br>JavaSVMWithSGDExample<br>模型评估<br>&emsp;&emsp;计算损失函数的方法也是有很多的，比如均方误差RMSE，MSE，R2等。这些参数都可以用来对线性回归模型进行评测。</p><p><strong>6.过拟合与欠拟合</strong><br>&emsp;&emsp;1.这也是机器学习过程中最为核心的问题。正规方程直接计算出最佳的结果，是不是就是最好的？其实也不是，数据之间的规律并不是稳定的，一般就不存在绝对的规律。机器学习的目的其实是要能够去对未来的数据进行预测，是一种模糊的行为。正规方程还有一个最重要的问题，就是他通常在训练集上表现良好，但是到验证集上表现就会差很多。这样的模型泛化能力不够，不能很好的体现未来数据的特征。这种问题就是过拟合现象，即机器学习从数据集中学到的规律过多。就像我们平常说的书呆子，学少了不好，学太多了同样也不好。<br>&emsp;&emsp;2.另外还有一种情况，如果数据的样本比较少，那机器学习学到的规律也会太不靠谱了。这就称为欠拟合现象。通常表现为算法在训练集和测试集上的表现都不太好。这就像我们常说的学渣，就是学习还不够。<br>&emsp;&emsp;3.过拟合和欠拟合现象是机器学习过程中绕不开的问题。而算法工程师很多的工作就是要综合评测算法和参数，在过拟合和欠拟合之间寻找最佳解。通常，对于欠拟合现象，可以通过加数据、加特征等手段来优化。而过拟合的优化就比较麻烦，通常需要做一些针对性的特征工程。</p><h1 id="6-逻辑回归与二分类"><a href="#6-逻辑回归与二分类" class="headerlink" title="6. 逻辑回归与二分类"></a>6. 逻辑回归与二分类</h1><p>&emsp;&emsp;二分类问题： 是否垃圾邮件、是否金融诈骗、是否虚假帐号。。。 这里即是用逻辑回归来解决二分类问题。<br><strong>1.逻辑回归原理：</strong><br>&emsp;&emsp;先对一组数据建立线性回归模型，h(w) = w1<em>x1 + w2</em>x2+…+b 然后用线性回归的输出作为逻辑回归的输入，将特征值映射到一个分类中，作为预测的目标值。例如：sigmoid激活函数：</p><p><img src="https://img-blog.csdnimg.cn/20191127172752241.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3JveWtpbmd3,size_16,color_FFFFFF,t_70" alt="sigmoid"></p><p>线性回归的输出作为逻辑回归的输入</p><p><img src="https://img-blog.csdnimg.cn/20191127172803822.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3JveWtpbmd3,size_16,color_FFFFFF,t_70" alt="sigmoid2"></p><p>例如，看下图的计算示例，逻辑回归的结果可以认为是样本的二分类概率。然后，同样通过损失函数来计算逻辑回归的模型性能。</p><p><img src="https://img-blog.csdnimg.cn/20191127172852955.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3JveWtpbmd3,size_16,color_FFFFFF,t_70" alt="逻辑回归"></p><p><strong>2.逻辑回归API：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sklearn.linear_model.LogisticRegrsssion(solver=&#x27;liblinear&#x27;,penalty=&#x27;l2&#x27;,C=1.0)</span><br><span class="line">- solver: 优化求解模式。 默认 liblinear , 还有sag 随机平均梯度下降</span><br><span class="line">- penalty: 正则化的种类。 l1 l2</span><br><span class="line">- C: 正则化力度</span><br></pre></td></tr></table></figure><p><strong>3.SparkDemo:</strong><br>&emsp;&emsp;JavaLogisticRegressionWithLBFGSExample</p><p><strong>4.逻辑回归模型评估：</strong><br>&emsp;&emsp;逻辑回归的结果只是一个二分类的概率，如有80%的可能是垃圾邮件。然而这种概率结果其实是很虚的，用模型评估拿到也是一个概率，闭上眼睛都选C也是一种概率。那要怎么评估模型的可信度呢？</p><p><strong>5.混淆矩阵、精确率(Precision)、召回率(Recall)</strong><br>&emsp;&emsp;在二分类任务中，预测结果与正确标记之间存在四种不同的组合，构成混淆矩阵(这个矩阵其实在多分类问题中也能建立，只是更加复杂)，通过混淆矩阵，可以对分类结果从多个不同的方面来进行评价。</p><table><thead><tr><th align="left">真实结果\预测结果</th><th>正例</th><th>假例</th></tr></thead><tbody><tr><td align="left">正例</td><td>真正例TP</td><td>为反例FN</td></tr><tr><td align="left">假例</td><td>伪正例FP</td><td>真反例TN</td></tr></tbody></table><p><strong>6.精确率Precision</strong>： 预测结果为正例样本中真实为正例的比例：</p><pre><code>  一批西瓜中，预测为好西瓜的瓜有多少真正是好西瓜。体现模型的准确性。--要把好西瓜挑出来</code></pre><p><img src="https://img-blog.csdnimg.cn/20191127172949142.png" alt="精确率"></p><p><strong>7.召回率Recall</strong>：真实为正例的样本中预测结果也为正例的比例：对正样本的区分能力。</p><pre><code>  一批西瓜中，所有真正为好西瓜的西瓜中有多少是被正确预测为好西瓜。体现模型对正样本的区分能力。--要把坏西瓜扔掉</code></pre><p><img src="https://img-blog.csdnimg.cn/20191127173019473.png" alt="召回率"></p><p>另外，还有其他更复杂的评估标准。如F1-Score，反映了模型的稳健型</p><p><img src="https://img-blog.csdnimg.cn/20191127173046640.png" alt="F1Score"></p><p><strong>8.混淆矩阵计算API：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">`sklearn.metrics.classification_report(y_true,y_pred,labels=[],target_name=None)`</span><br><span class="line">- `y_true : 真实目标值数组`</span><br><span class="line">- `y_pred: 估计器预测目标值`</span><br><span class="line">- `labels: 指定类别对应的数字`</span><br><span class="line">- `target_names: 目标类别名称`</span><br><span class="line">- `return： 每个类别精确率与召回率`</span><br><span class="line">  - `precision 精确率`</span><br><span class="line">  - `recall 召回率`</span><br><span class="line">  - `f1-scoreL 稳健度`</span><br><span class="line">  - `support 样本数`</span><br></pre></td></tr></table></figure><p>&emsp;&emsp;有了这些指标后，能够一定程度上评估模型的健康度。但是光有这些指标还不太够，例如在样本不均衡，正样本太多，负样本太少时，这些指标评估结果就不太可信。而为了能更准确的评估二分类模型，就引入了ROC曲线和AOC指标。</p><p><strong>9.ROC曲线和AUC指标</strong></p><p>首先需要了解TPRate和FPRate</p><ul><li><p>TPRate = TP / (TP+FN): 所有真实类别为1的样本中，预测类别也为1的比例</p></li><li><p>FPRate = FP/( FP + TN):所有真实类别为0的样本中，预测类别为1的比例</p><p>有这两个数据后，对于每一个分类器，就可以建立一条ROC曲线<br><img src="https://img-blog.csdnimg.cn/20191127173158282.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3JveWtpbmd3,size_16,color_FFFFFF,t_70" alt="ROC曲线"></p><p>而AUC指标就可以认为是ROC曲线下方的图形面积。</p><p>因此，</p></li><li><p>AUC指标的概率意义是随机取一对正负样本，正样本得分大于负样本的概率。</p></li><li><p>AUC指标的最小值是0.5，最大值是1，取值越大越好</p></li><li><p>AUC=1，就是完美分类器，采用这个预测模型时，不管设定什么阈值都能的出完美预测结果。但是，在绝大多数预测的场合，都不存在完美分类器。</p></li><li><p>0.5&lt;AUC&lt;1,优于随机猜测。这个分类器(模型)妥善设定阈值的话，能有预测价值。</p></li><li><p>如果AUC&lt;0.5，就会采用1-AUC来表示AUC的值，代表反向预测。</p></li><li><p>同时，AUC指标也能用于比较多个不同的分类器的性能。</p></li></ul><p><strong>10.AUC指标计算API：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">`sklearn.metrics.roc_auc_score(y_true,y_score)`</span><br><span class="line">- `计算ROC曲线面积，即AUC值`</span><br><span class="line">- `y_true: 每个样本的真是类别，必须为0-反例，1-正例 标记`</span><br><span class="line">- `y_score:预测得分，可以是正类的估计概率、可信值或者分类器方法的返回值`</span><br></pre></td></tr></table></figure><p><strong>11.Spark计算AUC示例：</strong></p><p>见JavaLBFGSExample中一段示例代码</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// Get evaluation metrics.</span><br><span class="line">    BinaryClassificationMetrics metrics =</span><br><span class="line">      new BinaryClassificationMetrics(scoreAndLabels.rdd());</span><br><span class="line">    double auROC = metrics.areaUnderROC();</span><br></pre></td></tr></table></figure><p><strong>12.AUC总结</strong></p><ul><li><p>AUC只能用来评估二分类问题</p></li><li><p>AUC非常适合评价样本不均衡时的分类器性能。</p></li></ul><h1 id="7-无监督学习-K-means算法"><a href="#7-无监督学习-K-means算法" class="headerlink" title="7. 无监督学习-K-means算法"></a>7. 无监督学习-K-means算法</h1>]]></content>
    
    
    <summary type="html">&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;机器学习适用的数据为具有特征值(属性，label)和目标值(结果,point)的数据集。通过从历史数据集中学习经验，建立模型，从而达到预测新特征值对应的目标值的效果。因此在数据方面，越见多识广的数据集(样本集越大越全)，越能进行更可信的预测(越准确的预测)。&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    
    <category term="机器学习" scheme="https://aj-web.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="算法" scheme="https://aj-web.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
    <category term="特征工程、推荐系统" scheme="https://aj-web.github.io/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E3%80%81%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统-1：理论基础</title>
    <link href="https://aj-web.github.io/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/"/>
    <id>https://aj-web.github.io/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/</id>
    <published>2021-10-20T16:00:00.000Z</published>
    <updated>2021-12-29T06:18:55.176Z</updated>
    
    <content type="html"><![CDATA[<hr><blockquote><p>相信大家都在网上买过东西，那么各种各样的商城都会有类似猜你喜欢的推荐，不知道大家觉得这些推荐准嘛？可能准确率会因人而异，但是可以确定是，这种推荐系统对于提高销售额，一定是有帮助的，今天就来了解一下推荐系统</p></blockquote><span id="more"></span><h1 id="1-推荐系统功能介绍"><a href="#1-推荐系统功能介绍" class="headerlink" title="1. 推荐系统功能介绍"></a>1. 推荐系统功能介绍</h1><h2 id="1-1-推荐系统"><a href="#1-1-推荐系统" class="headerlink" title="1.1 推荐系统"></a>1.1 推荐系统</h2><p>&emsp;&emsp;推荐系统是利用电子商务网站向客户提供商品信息和建议，帮助用户决定应该购买什么产品，模拟销售人员帮助客户完成购买过程。个性化推荐是根据用户的兴趣特点和购买行为，向用户推荐用户感兴趣的信息和商品</p><h2 id="1-2-定义"><a href="#1-2-定义" class="headerlink" title="1.2 定义"></a>1.2 定义</h2><p>&emsp;&emsp;推荐系统有3个重要的模块：用户建模模块、推荐对象建模模块、推荐算法模块。通用的推荐系统模型流程如图。推荐系统把用户模型中兴趣需求信息和推荐对象模型中的特征信息匹配，同时使用相应的推荐算法进行计算筛选，找到用户可能感兴趣的推荐对象，然后推荐给用户。<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9A%E4%B9%89%E5%9B%BE.png" alt="推荐系统"></p><h1 id="2-推荐系统实现"><a href="#2-推荐系统实现" class="headerlink" title="2. 推荐系统实现"></a>2. 推荐系统实现</h1><h2 id="2-1-简单推荐系统思路？"><a href="#2-1-简单推荐系统思路？" class="headerlink" title="2.1 简单推荐系统思路？"></a>2.1 简单推荐系统思路？</h2><p>例如：<br>1.把每天打折的物品进行推荐？<br>2.在订单表中找销售量最靠前的产品？<br>&emsp;&emsp;上述也是比较简单的实现思路，但是并不智能，不能根据每个人的喜好来推荐，各大主流的购物网也明显不是采用这种方式来实现</p><p>&emsp;&emsp;所以要选择合适的推荐系统，我们需要了解推荐系统的作用：<br>&emsp;&emsp;推荐系统要能够根据用户的信息需求，兴趣爱好等，将用户感兴趣的信息、产品等推荐给用户的个性化信息推荐系统。<br>&emsp;&emsp;站在电商网站的角度，只是从销量或者打折这样单方面进行推荐，这样的推荐系统肯定是起不到吸引客户消费的目的。所以，好的推荐系统必须考虑到用户的喜好以及产品的特点。用户经常浏览以及购买什么物品，购买产品最多的人是年轻人还是老年人？那电商网站怎么知道用户的喜好，以及产品的受众呢？这就必须要基于大量用户和产品的数据，所以本次的推荐系统就是来带大家一起处理，收集，计算这些大量的用户以及产品的数据，通过机器学习的方式，形成一个有价值的推荐系统</p><h2 id="2-2-推荐系统核心理解"><a href="#2-2-推荐系统核心理解" class="headerlink" title="2.2 推荐系统核心理解"></a>2.2 推荐系统核心理解</h2><p>&emsp;&emsp;推荐系统在我们实际生活中，可以有很多衍生场景。那这些推荐系统的核心到底是什么呢？其实推荐系统的真正核心可以理解为一个矩阵求解的数学问题。<br>&emsp;&emsp;比如，网站向用户推荐商品，往往要基于用户以以往的浏览记录或者评价记录，而这些记录就可以抽象为（userid，productid，score）这样的一个向量结构，score可以是一个任意的数字，比如在这里表示用户的浏览次数，也可以是一个0或1的值，表示用户与商品之间是否建立了关系，比如是否购买过商品。而这样一些数据往往是比较零散的。当我们需要将这些数据整体进行梳理，就会以userid为一列，productid为一行，整理成这样一个矩阵。<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/%E7%94%A8%E6%88%B7%E5%95%86%E5%93%81%E5%90%91%E9%87%8F%E7%9F%A9%E9%98%B5.png" alt="用户商品向量矩阵"></p><p>&emsp;&emsp;一个向量数据,就代表了矩阵中的一个点，在这个矩阵中，数据通常是比较稀疏的，称为稀疏矩阵，而推荐算法要做的，就是将这些矩阵中的空白点，以某一种方法进行部分填充或者全部填充，每填充一个点，就代表向用户推荐这个产品的一个推荐指数，然后选举推荐指数比较高的产品推荐给客户。</p><h2 id="2-3-推荐系统效果评价"><a href="#2-3-推荐系统效果评价" class="headerlink" title="2.3 推荐系统效果评价"></a>2.3 推荐系统效果评价</h2><p>&emsp;&emsp;针对上面想法，当我们有了数据之后，我们就可以把推荐系统这个抽象的理论问题，转换成一个具体的数学问题，但是这个问题并没有标准的答案，我们可以往里面填写任意的数字，即使当我们运用机器学习来预测，不同的模型，算法，也会预测出不同的结果，但是在众多的结果之中，总会有一些结果会更加适合大众的口味，虽然不同的推荐系统没有明确的分数来表示他的好坏，但是，最重他们还是会体现出不同层次的好坏。<br>&emsp;&emsp;那么为什么会这样呢？从数学的角度来看待这个问题，就是因为我们自己设计的推荐系统没有很好的利用已有的数据，没有从已有数据中”学习”到内在的规律。而好的推荐系统则是通过机器学习很好的挖掘出了已有数据的一些内在规律，这些内在规律可以体现为每个用户的兴趣爱好，每个产品的最佳受众等等很多规律，甚至于很多人类无法描述的规律。<br>&emsp;&emsp;比如最经典的啤酒和尿不湿要放在一起售卖的问题，这是一个机器学习中的经典故事，你可以强行做一些解释，但是总是很难接触到本质。所欲，对于推荐系统的评价需要基于非常多的维度进行综合评价。大致可以分为以下几类：</p><ul><li><p>基于常识的评判标准<br>&emsp;&emsp;从上面我们已经知道了，推荐系统其实只是一个数字游戏，但是我们的业务不可能是简单的数字游戏。有一些推荐的结果，我们是可以直接从业务上判断是好是坏的，其中坏的推荐的比例，就可以作为对推荐系统一个评判的标准。</p><p>&emsp;&emsp;比如，对于天猫，淘宝这类电商网站，给用户推荐他已经购买过的商品，往往就不是一个比较好的结果。但是推荐已经购买过的商品的周边商品，这个结果就比较好。就比如用户购买了一个鼠标，用户再去购买鼠标的概率就比较低，但是如果推荐一个配套的鼠标垫，鼠标贴，用户去购买的可能性就会比较高。<br>&emsp;&emsp;对于一个音乐类的内容推荐系统，如果用户是某一个歌星的粉丝，那么再去给他推荐这个歌星的其他的歌或者专辑，意义就不是那么大，因为这些内容用户通常都会主动搜索。再比如对一个新闻类的内容推荐系统，如果推荐给用户的新闻包含了很多”过时”的内容，或者推荐很久之前的帖子，那么显然也不是一个好的推荐</p></li><li><p>基于指标的评判标准<br>&emsp;&emsp;通常对于业务系统的评价，还是会回归到业务的本身。所以对于推荐系统最常见的评判标准，还是通过一定的业务指标来衡量。例如常见的PV、UV、用户留存率、转化率等等，通过比对上推荐系统之前和之后的指标数据，来衡量一个推荐系统是不是有效，或者拿推荐系统优化前后的数据进行比较，来看推荐系统的优化是否有效。比如对于猫眼这样的购票系统，最直接的衡量标准就是推荐系统带来的流量和收入的增长<br>&emsp;&emsp;另外，对于推荐系统，还有⼀个⾮常重要的指标就是推荐产品的覆盖率，也就是推荐出来的产品应该要越丰富越  好。这是为什么呢？这就涉及到了电商的⼀个根本性的理论模型-“⻓尾经济模型”。商品的交易⾏为，通常都会遵循<br>⼀个普遍性的2-8理论，即80%的利润出⾃于20%的商品。⽐如我们去超市购物，通常也都⽐较喜欢购买最热⻔的， 品牌印象⼒⼤的商品。所以当我们以产品为X轴，产品带来的利润为Y轴，经过整理通常都能得到⼀个这样的正态分布图<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/%E5%95%86%E5%93%81%E5%88%A9%E6%B6%A6%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83.png" alt="商品利润正态分布图">   </p></li><li><p>基于机器学习的评判标准<br>&emsp;&emsp;通常推荐系统需要结合⼤量的业务数据，通过对历史数据的挖掘、分析，归纳出⽤⼾与产品之间的⼀些关系。这些  关系通常过于隐晦，有些是能够进⾏解释的特征，⽐如⽤⼾的爱好、产品的受众特点等。但是往往还有很多隐藏的关系是⽆法⽤简单的常理来解释的。这些关系就要通过机器学习的算法来进⾏深⼊挖掘。最终通过这些分析，所以现在业界普遍的推荐系统都是基于机器学习算法来完成的。⽽每⼀个机器学习的算法，都会有他⾃⼰的评判指标和优化⽅式</p></li></ul><h1 id="3-推荐系统-机器学习基础"><a href="#3-推荐系统-机器学习基础" class="headerlink" title="3 推荐系统-机器学习基础"></a>3 推荐系统-机器学习基础</h1><h2 id="3-1-机器学习的起源："><a href="#3-1-机器学习的起源：" class="headerlink" title="3.1 机器学习的起源："></a>3.1 机器学习的起源：</h2><ul><li><strong>机器学习的5个学派：</strong>  </li></ul><ol><li><p>符号主义(Symbolism)，也称为逻辑主义，强调认知即计算，通过对符号的演绎和逆演绎进行结果预测，最喜欢的算法是：规则和决策树。  </p></li><li><p>连接主义(Connectionists)，强调的是从仿⽣学的⻆度，对大脑进行仿真，使用概率矩阵和加权神经元来动态地识别和归纳模式，最喜欢的算法是：深度学习<br>。  </p></li><li><p>⾏为主义(Analogizer)，强调新旧知识间的相似性 ，代表算法:核机器（Kernel machines）、近邻算法（Nearest Neightor）</p></li><li><p>贝叶斯派(Bayesians)：获取发生的可能性来进行概率推理，强调主观概率估计，发生概率修正，最优决策，最喜欢的算法是：朴素贝叶斯或马尔可夫</p></li><li><p>进化主义(Evolutionaries):强调对进化进行模拟，使用遗传算法和遗传编程</p></li></ol><ul><li><strong>机器学习的应⽤领域是⾮常多的，⼤体上，可以分为三个主要的⽅向：</strong></li></ul><ol><li>传统预测：主要⽤在数据挖掘，预测领域。典型的应⽤场景： 店铺销量预测、房价预测、垃圾邮件安全监测等。包括我们这个课程的推荐系统，其实⼤体上也可以分到这⼀类。当然，这也并不是绝对的。基于神经⽹络 的推荐系统也是有很多落地实现的</li><li>图像识别：典型应⽤场景：⾃动驾驶、⼈脸识别、涉⻩图⽚视频过滤等</li><li>⾃然语⾔处理：典型应⽤场景：⽂本分类、聊天机器⼈、智能客服、⽂本翻译等。其实我们能感觉到，早期的  百度中英⽂翻译就⾮常难懂，语法⾮常混乱。但是现在百度中英⽂翻译就相当⼈性化了，语法也⾮常⾃然。这 其中就有深度学习参与其中。</li></ol><h2 id="3-2-机器学习数据"><a href="#3-2-机器学习数据" class="headerlink" title="3.2 机器学习数据"></a>3.2 机器学习数据</h2><p>&emsp;&emsp;机器学习有三个关键词： 数据，模型，预测。机器学习强调从历史数据中⾃动学习，对数据之间的规律进⾏归<br>纳，形成模型，然后⽤模型来对实际问题进⾏预测。这个过程跟⼈类理解⼀个事物的过程是很类似的。回想⼀下，⼈类去分辨猫和狗、或者预测房价未来的⾛势，其实也是这样⼀个过程。⼈需要从⼤量的⽇常⽣活经验中归纳出⼀系列的规律，然后在⾯临具体问题时，就可以从众多规律中找到最优化的规律，来解决⽇常问题。<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84.png" alt="机器学习数据结构"></p><p>&emsp;&emsp;⽐如像这样⼀个房价数据，每⼀⾏数据称之为⼀个样本。多个样本就构成了⼀个数据集。⽽在每个样本当中，前⾯  部分房⼦的各个属性构成了特征值，代表样本的各个数据特征。⽽最后的⽬标值相当于是样本的结果。⽽机器学习的  过程就是要从已有的房价数据中学习到房价之间的规律，然后以后再来⼀个房⼦，我们就可以根据房⼦的这些属性，  预测他的房价是多少。⽽在数据集中，特征值是必不可少的，⼀般就是原始数据。⽽⽬标值有可能需要通过对数据进<br>⾏处理来获得，但是有些数据集是可以没有⽬标值的。</p><h2 id="3-3-机器学习算法分类"><a href="#3-3-机器学习算法分类" class="headerlink" title="3.3 机器学习算法分类"></a>3.3 机器学习算法分类</h2><p>&emsp;&emsp;机器学习涉及到⾮常多的数学算法。对这些数学算法，进⾏简单分类。<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%88%86%E7%B1%BB.png" alt="机器学习算法分类"><br>&emsp;&emsp;1.监督学习算法 (Supervised Algorithms）:在监督学习训练过程中，可以由训练数据集学到或建立一个模式（函数 / learning model），并依此模式推测新的实例。该算法要求特定的输入/输出，首先需要决定使用哪种数据作为范例。例如，文字识别应用中一个手写的字符，或一行手写文字。主要算法包括神经网络、支持向量机、最近邻居法、朴素贝叶斯法、决策树等。<br>&emsp;&emsp;2.无监督学习算法 (Unsupervised Algorithms):这类算法没有特定的目标输出，算法将数据集分为不同的组。(聚类)<br>&emsp;&emsp;3.强化学习算法 (Reinforcement Algorithms):强化学习普适性强，主要基于决策进行训练，算法根据输出结果（决策）的成功或错误来训练自己，通过大量经验训练优化后的算法将能够给出较好的预测。类似有机体在环境给予的奖励或惩罚的刺激下，逐步形成对刺激的预期，产生能获得最大利益的习惯性行为。在运筹学和控制论的语境下，强化学习被称作“近似动态规划”（approximate dynamic programming，ADP）。</p><h2 id="3-4-怎么获取数据集？"><a href="#3-4-怎么获取数据集？" class="headerlink" title="3.4 怎么获取数据集？"></a>3.4 怎么获取数据集？</h2><p>&emsp;&emsp;在实际业务中，这些有⽤的数据集成本巨⼤，甚⾄可能包含了很多核⼼的商业机密。那在学习阶段，我们要怎么去  获得有价值的数据集呢？主要还是通过直接使⽤别⼈维护好的数据集。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">⾏业内有很多科研⼈员都维护了很多质量⾮常⾼的开源数据集。例如python的sklearn框架就集成了⼀部分常⽤的数据集。</span><br><span class="line"></span><br><span class="line">参⻅pycharm中的Demo: sklearn_datasets.py，加载sklearn本地的Iris鸢尾花数据集，还有也加载了</span><br><span class="line">Boston波斯顿房价数据集。这两个数据集是机器学习领域最为经典的数据集。Iris就是分类问题数据集， Boston则是回归问题数据集。</span><br><span class="line"></span><br><span class="line">⽽在java领域，可以使⽤Spark的mllib包来做机器学习。也可以将这些csv⽂件读到spark当中。参⻅SparkDemo中的</span><br><span class="line">LoadDataDemo。</span><br></pre></td></tr></table></figure><p>UCI： <a href="http://archive.ics.uci.edu/ml/">http://archive.ics.uci.edu/ml/</a>  这个⽹站上维护了很多经典的数据集。<br>kaggle： <a href="https://www.kaggle.com/">https://www.kaggle.com/</a>  ⼀个综合性的机器学习竞赛平台。上⾯会开放很多数据集，开展很多机器学习的竞赛。有很多都是⼀些公司⾃⼰处理不了的实际数据，数据集的质量通常都是⽐较⾼的。同时也有很多别⼈分享  的基础教程以及算法分享，也都是⾮常不错的学习资料。</p><p>&emsp;&emsp;关于机器学习，有⼀本⾮常经典的⼊⻔资料，就是周志华的《机器学习》，俗称为西⽠书。因封⾯有很多西⽠，并且全篇很     多问题都从西⽠谈起⽽得名。<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/%E8%A5%BF%E7%93%9C%E4%B9%A6.png" alt="西瓜书"></p><p>⽽关于深度学习，也有⼀本⾮常经典的资料，名字就叫做《Deep leaning》，深度学习。俗称为花书，因封⾯有⾮常多的花<br>⽽得名。<br><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/%E8%8A%B1%E4%B9%A6.jpg" alt="花书"></p><h1 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h1><p><img src="https://raw.githubusercontent.com/aj-web/picturebed/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%80%BB%E7%BB%93.png" alt="机器学习在推荐系统中的应用总结"></p>]]></content>
    
    
    <summary type="html">&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;相信大家都在网上买过东西，那么各种各样的商城都会有类似猜你喜欢的推荐，不知道大家觉得这些推荐准嘛？可能准确率会因人而异，但是可以确定是，这种推荐系统对于提高销售额，一定是有帮助的，今天就来了解一下推荐系统&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    
    <category term="机器学习" scheme="https://aj-web.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="推荐系统" scheme="https://aj-web.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>数据结构-1：算法入门</title>
    <link href="https://aj-web.github.io/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E5%85%A5%E9%97%A8/"/>
    <id>https://aj-web.github.io/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E5%85%A5%E9%97%A8/</id>
    <published>2021-09-26T16:00:00.000Z</published>
    <updated>2021-12-29T06:18:55.188Z</updated>
    
    <content type="html"><![CDATA[<hr><blockquote><h1 id="1-为什么要学习数据结构与算法"><a href="#1-为什么要学习数据结构与算法" class="headerlink" title="1.为什么要学习数据结构与算法"></a>1.为什么要学习数据结构与算法</h1><ol><li>面试必问</li><li>加深对集合类的理解和使用</li><li>架构师必备，写出框架级的代码；API，写出开源级代码，同时应对中年危机</li><li>提升自己的能力，不被行业淘汰：H5，小程序。红黑树，几十年的时间。B+Tree</li></ol></blockquote><span id="more"></span><h1 id="2-什么是数据结构与算法"><a href="#2-什么是数据结构与算法" class="headerlink" title="2.什么是数据结构与算法"></a>2.什么是数据结构与算法</h1><ol><li>数据存储于内存时，决定了数据顺序和位置关系的便是数据结构</li><li>算法就是解决问题的最优解</li></ol><h1 id="3-算法的特点"><a href="#3-算法的特点" class="headerlink" title="3.算法的特点"></a>3.算法的特点</h1><ol><li>五个特征：有穷性、确定性、可行性、有输入、有输出</li><li>设计原则：正确性、可读性、健壮性、写出代码很少有bug，而且系统比较稳定</li></ol><h1 id="4-时间复杂度与空间复杂度"><a href="#4-时间复杂度与空间复杂度" class="headerlink" title="4.时间复杂度与空间复杂度"></a>4.时间复杂度与空间复杂度</h1><ol><li>时间复杂度计算意义：程序运行的时间</li><li> 时间复杂度表示方法：  </li></ol><ul><li>常数：O(1) 1表示是常数，所有能确定的数字我们都用O（1），O(1000)=&gt;o(1)</li><li>对数：O(logn),O(nlogn)</li><li>线性：O(n)</li><li>线性对数：O(nlogn)</li><li>平方：O(n^2)</li><li>N次方：O(n^n)</li></ul><ol start="3"><li>时间复杂度如何分析：<br>（1）找for while 递归。而且要找循环量最大的那一段<br>（2）同级循环怎么计算</li></ol><ol start="4"><li>如何找时间复杂度：</li></ol><ul><li>找到有循环的地方</li><li>找有网络请求（RPC，远程调用，分布式，数据库请求）的地方。就是测试时间：log打印，计算平均时间。</li></ul><p>(<img src="https://raw.githubusercontent.com/aj-web/picturebed/master/%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%E6%9B%B2%E7%BA%BF%E5%9B%BE.png" alt="时间复杂度">)</p><p>5.从图我们可以的出最终的结论：O(1)&gt;O(logn)&gt;O(n)&gt;O(nlogn)&gt;O(n^2)&gt;O(n^x)，从O(1)&gt;O(logn)&gt;O(n)&gt;O(nlogn)，这些效果都是很好的。几乎优化的空间不是很大，但是后面两个时间复杂度是尽可能的优化下，我们优化最终的目标就是往O(1)的方向接近。</p><hr><h2 id="例题：判断一个数是否是2的N次方"><a href="#例题：判断一个数是否是2的N次方" class="headerlink" title="例题：判断一个数是否是2的N次方"></a>例题：判断一个数是否是2的N次方</h2><ol start="0"><li>传统思路：对这个数取余 n%2==0?,如果等于0就是2的N次方</li><li>优化思路：采取2进制进行与计算,<br>原理：1–&gt;01   2–&gt;10   3–&gt;011  4–&gt;100<br>那么我们可以发现： 3&amp;2 !=0 4&amp;3==0<br>也就是，if(n&amp;(n-1)==0),那么n就是2的次方数</li></ol>]]></content>
    
    
    <summary type="html">&lt;hr&gt;
&lt;blockquote&gt;
&lt;h1 id=&quot;1-为什么要学习数据结构与算法&quot;&gt;&lt;a href=&quot;#1-为什么要学习数据结构与算法&quot; class=&quot;headerlink&quot; title=&quot;1.为什么要学习数据结构与算法&quot;&gt;&lt;/a&gt;1.为什么要学习数据结构与算法&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;面试必问&lt;/li&gt;
&lt;li&gt;加深对集合类的理解和使用&lt;/li&gt;
&lt;li&gt;架构师必备，写出框架级的代码；API，写出开源级代码，同时应对中年危机&lt;/li&gt;
&lt;li&gt;提升自己的能力，不被行业淘汰：H5，小程序。红黑树，几十年的时间。B+Tree&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    
    <category term="数据结构" scheme="https://aj-web.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    <category term="算法" scheme="https://aj-web.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
